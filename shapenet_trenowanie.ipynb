{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WjtBCQQ9gzIR",
    "outputId": "b38765ab-4778-49e6-b045-3f5dc45a10fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 761 kB 41.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 62.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 386 kB 63.7 MB/s \n",
      "\u001b[?25h  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 8.7.0 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipdb -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "3T1lRg05bJCu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import ipdb\n",
    "\n",
    "class STN3d(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(STN3d, self).__init__()\n",
    "        self.debug1='debug1'\n",
    "        print('debug1')\n",
    "        #ipdb.set_trace(context=5)\n",
    "        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n",
    "        print('debug2')\n",
    "        #ipdb.set_trace(context=5)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 9)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64) #To jest ta normalizacja odchylenia i sredniej (?1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #ipdb.set_trace(context=5)\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024) #to jest jakiś resize...(?2)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        #ipdb.set_trace(context=5)\n",
    "        iden = Variable(torch.from_numpy(np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32))).view(1, 9).repeat(\n",
    "            batchsize, 1)\n",
    "        if x.is_cuda:\n",
    "            iden = iden.cuda()\n",
    "        x = x + iden\n",
    "        x = x.view(-1, 3, 3)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "pgQ0ziwrkyz0"
   },
   "outputs": [],
   "source": [
    "class STNkd(nn.Module):\n",
    "    def __init__(self, k=64):\n",
    "        super(STNkd, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k*k)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1,self.k*self.k).repeat(batchsize,1)\n",
    "        if x.is_cuda:\n",
    "            iden = iden.cuda()\n",
    "        x = x + iden\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "QXhWk4qBk1LU"
   },
   "outputs": [],
   "source": [
    "class PointNetfeat(nn.Module):\n",
    "    def __init__(self, global_feat = True, feature_transform = False):\n",
    "        super(PointNetfeat, self).__init__()\n",
    "        self.stn = STN3d(channel=3)\n",
    "        #self.stn = STN3d()\n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.global_feat = global_feat\n",
    "        self.feature_transform = feature_transform\n",
    "        if self.feature_transform:\n",
    "            self.fstn = STNkd(k=64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_pts = x.size()[2]\n",
    "        trans = self.stn(x)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = torch.bmm(x, trans)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        if self.feature_transform:\n",
    "            trans_feat = self.fstn(x)\n",
    "            x = x.transpose(2,1)\n",
    "            x = torch.bmm(x, trans_feat)\n",
    "            x = x.transpose(2,1)\n",
    "        else:\n",
    "            trans_feat = None\n",
    "\n",
    "        pointfeat = x\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "        if self.global_feat:\n",
    "            return x, trans, trans_feat\n",
    "        else:\n",
    "            x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
    "            return torch.cat([x, pointfeat], 1), trans, trans_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "t2XnqXHNk5sU"
   },
   "outputs": [],
   "source": [
    "class PointNetCls(nn.Module):\n",
    "    def __init__(self, k=2, feature_transform=False):\n",
    "        super(PointNetCls, self).__init__()\n",
    "        self.feature_transform = feature_transform\n",
    "        self.feat = PointNetfeat(global_feat=True, feature_transform=feature_transform)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, trans, trans_feat = self.feat(x)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.dropout(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1), trans, trans_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "REANOaTqk7dm"
   },
   "outputs": [],
   "source": [
    "class PointNetDenseCls(nn.Module):\n",
    "    def __init__(self, k = 2, feature_transform=False):\n",
    "        super(PointNetDenseCls, self).__init__()\n",
    "        self.k = k\n",
    "        self.feature_transform=feature_transform\n",
    "        self.feat = PointNetfeat(global_feat=False, feature_transform=feature_transform)\n",
    "        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        n_pts = x.size()[2]\n",
    "        x, trans, trans_feat = self.feat(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.conv4(x)\n",
    "        x = x.transpose(2,1).contiguous()\n",
    "        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n",
    "        x = x.view(batchsize, n_pts, self.k)\n",
    "        return x, trans, trans_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "xrpWG0vJqcya"
   },
   "outputs": [],
   "source": [
    "def feature_transform_regularizer(trans):\n",
    "    d = trans.size()[1]\n",
    "    batchsize = trans.size()[0]\n",
    "    I = torch.eye(d)[None, :, :]\n",
    "    if trans.is_cuda:\n",
    "        I = I.cuda()\n",
    "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUsJ_DmkbOAQ",
    "outputId": "600d4bab-18a4-48ed-f62e-96658b3ca787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug1\n",
      "debug2\n",
      "debug1\n",
      "debug2\n"
     ]
    }
   ],
   "source": [
    "stn=STN3d(channel=3)\n",
    "pnc=PointNetCls(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d3OYAkjxbW3J"
   },
   "outputs": [],
   "source": [
    "data=torch.rand(2,3,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "mRDlTayEbs3S"
   },
   "outputs": [],
   "source": [
    "dataTrans=stn.forward(data)\n",
    "dataTrans2, trans=pnc.forward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExXS9X0j084V",
    "outputId": "309a64a0-a27d-47e6-a6e5-e5a4127befc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTrans.shape\n",
    "dataTrans2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEopUc4tn7rS",
    "outputId": "12c35d47-2832-48c2-c227-b0773005c9d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting plyfile\n",
      "  Downloading plyfile-0.7.4-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.8/dist-packages (from plyfile) (1.21.6)\n",
      "Installing collected packages: plyfile\n",
      "Successfully installed plyfile-0.7.4\n"
     ]
    }
   ],
   "source": [
    "!pip install plyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kTY2vmIvo-BU",
    "outputId": "9f0e6841-239c-49d3-a058-f2bebc75417e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "echo \"Airplane\t4\n",
    "Bag\t2\n",
    "Cap\t2\n",
    "Car\t4\n",
    "Chair\t4\n",
    "Earphone\t3\n",
    "Guitar\t3\n",
    "Knife\t2\n",
    "Lamp\t4\n",
    "Laptop\t2\n",
    "Motorbike\t6\n",
    "Mug\t2\n",
    "Pistol\t3\n",
    "Rocket\t3\n",
    "Skateboard\t3\n",
    "Table\t3\" > ./misc/num_seg_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmooTYS_psF2",
    "outputId": "16693715-29ce-425c-b0d6-b727fc0f8a30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "echo \"airplane\t0\n",
    "bathtub\t1\n",
    "bed\t2\n",
    "bench\t3\n",
    "bookshelf\t4\n",
    "bottle\t5\n",
    "bowl\t6\n",
    "car\t7\n",
    "chair\t8\n",
    "cone\t9\n",
    "cup\t10\n",
    "curtain\t11\n",
    "desk\t12\n",
    "door\t13\n",
    "dresser\t14\n",
    "flower_pot\t15\n",
    "glass_box\t16\n",
    "guitar\t17\n",
    "keyboard\t18\n",
    "lamp\t19\n",
    "laptop\t20\n",
    "mantel\t21\n",
    "monitor\t22\n",
    "night_stand\t23\n",
    "person\t24\n",
    "piano\t25\n",
    "plant\t26\n",
    "radio\t27\n",
    "range_hood\t28\n",
    "sink\t29\n",
    "sofa\t30\n",
    "stairs\t31\n",
    "stool\t32\n",
    "table\t33\n",
    "tent\t34\n",
    "toilet\t35\n",
    "tv_stand\t36\n",
    "vase\t37\n",
    "wardrobe\t38\n",
    "xbox\t39\" > ./misc/modelnet_id.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTrMcXnQvYm6"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "wget http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNpd19II--vf"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "wget http://modelnet.cs.princeton.edu/ModelNet40.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kr-R74wmyaka"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "wget https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_segmentation_benchmark_v0.zip --no-check-certificate\n",
    "unzip shapenetcore_partanno_segmentation_benchmark_v0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WJ2txnsxQkY"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "unzip ModelNet10.zip\n",
    "unzip ModelNet40.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ibFQKLxgKIpz",
    "outputId": "0d99451f-7467-41a3-ab25-6108eaf92140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-13 11:05:14--  https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip\n",
      "Resolving shapenet.cs.stanford.edu (shapenet.cs.stanford.edu)... 171.67.77.19\n",
      "Connecting to shapenet.cs.stanford.edu (shapenet.cs.stanford.edu)|171.67.77.19|:443... connected.\n",
      "WARNING: cannot verify shapenet.cs.stanford.edu's certificate, issued by ‘CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US’:\n",
      "  Issued certificate has expired.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 435212151 (415M) [application/zip]\n",
      "Saving to: ‘modelnet40_ply_hdf5_2048.zip’\n",
      "\n",
      "modelnet40_ply_hdf5 100%[===================>] 415.05M  13.5MB/s    in 31s     \n",
      "\n",
      "2022-12-13 11:05:45 (13.6 MB/s) - ‘modelnet40_ply_hdf5_2048.zip’ saved [435212151/435212151]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "wget https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WI9bEcH7KbAb",
    "outputId": "43ea0028-6c6d-4a6f-8563-bde00e6de7c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  modelnet40_ply_hdf5_2048.zip\n",
      "   creating: modelnet40_ply_hdf5_2048/\n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train_2_id2file.json  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train2.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train4.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train1.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/train_files.txt  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train_4_id2file.json  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_test1.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_test0.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_test_1_id2file.json  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train_1_id2file.json  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train_0_id2file.json  \n",
      "  inflating: modelnet40_ply_hdf5_2048/test_files.txt  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train0.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_test_0_id2file.json  \n",
      "  inflating: modelnet40_ply_hdf5_2048/shape_names.txt  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train3.h5  \n",
      "  inflating: modelnet40_ply_hdf5_2048/ply_data_train_3_id2file.json  \n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "unzip modelnet40_ply_hdf5_2048.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "p9WzFLbwoTkD"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm \n",
    "import json\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "def get_segmentation_classes(root):\n",
    "    catfile = os.path.join(root, 'synsetoffset2category.txt')\n",
    "    cat = {}\n",
    "    meta = {}\n",
    "\n",
    "    with open(catfile, 'r') as f:\n",
    "        for line in f:\n",
    "            ls = line.strip().split()\n",
    "            cat[ls[0]] = ls[1]\n",
    "\n",
    "    for item in cat:\n",
    "        dir_seg = os.path.join(root, cat[item], 'points_label')\n",
    "        dir_point = os.path.join(root, cat[item], 'points')\n",
    "        fns = sorted(os.listdir(dir_point))\n",
    "        meta[item] = []\n",
    "        for fn in fns:\n",
    "            token = (os.path.splitext(os.path.basename(fn))[0])\n",
    "            meta[item].append((os.path.join(dir_point, token + '.pts'), os.path.join(dir_seg, token + '.seg')))\n",
    "    \n",
    "    with open('./misc/num_seg_classes.txt', 'w') as f:\n",
    "        for item in cat:\n",
    "            datapath = []\n",
    "            num_seg_classes = 0\n",
    "            for fn in meta[item]:\n",
    "                datapath.append((item, fn[0], fn[1]))\n",
    "\n",
    "            for i in tqdm(range(len(datapath))):\n",
    "                l = len(np.unique(np.loadtxt(datapath[i][-1]).astype(np.uint8)))\n",
    "                if l > num_seg_classes:\n",
    "                    num_seg_classes = l\n",
    "\n",
    "            print(\"category {} num segmentation classes {}\".format(item, num_seg_classes))\n",
    "            f.write(\"{}\\t{}\\n\".format(item, num_seg_classes))\n",
    "\n",
    "def gen_modelnet_id(root):\n",
    "    classes = []\n",
    "    with open(os.path.join(root, 'train.txt'), 'r') as f:\n",
    "        for line in f:\n",
    "            classes.append(line.strip().split('/')[0])\n",
    "    classes = np.unique(classes)\n",
    "    with open('./misc/modelnet_id.txt', 'w') as f:\n",
    "        for i in range(len(classes)):\n",
    "            f.write('{}\\t{}\\n'.format(classes[i], i))\n",
    "\n",
    "class ShapeNetDataset(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 npoints=2500,\n",
    "                 classification=False,\n",
    "                 class_choice=None,\n",
    "                 split='train',\n",
    "                 data_augmentation=True):\n",
    "        self.npoints = npoints\n",
    "        self.root = root\n",
    "        self.catfile = os.path.join(self.root, 'synsetoffset2category.txt')\n",
    "        self.cat = {}\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.classification = classification\n",
    "        self.seg_classes = {}\n",
    "        \n",
    "        with open(self.catfile, 'r') as f:\n",
    "            for line in f:\n",
    "                ls = line.strip().split()\n",
    "                self.cat[ls[0]] = ls[1]\n",
    "        #print(self.cat)\n",
    "        if not class_choice is None:\n",
    "            self.cat = {k: v for k, v in self.cat.items() if k in class_choice}\n",
    "\n",
    "        self.id2cat = {v: k for k, v in self.cat.items()}\n",
    "\n",
    "        self.meta = {}\n",
    "        splitfile = os.path.join(self.root, 'train_test_split', 'shuffled_{}_file_list.json'.format(split))\n",
    "        #from IPython import embed; embed()\n",
    "        filelist = json.load(open(splitfile, 'r'))\n",
    "        for item in self.cat:\n",
    "            self.meta[item] = []\n",
    "\n",
    "        for file in filelist:\n",
    "            _, category, uuid = file.split('/')\n",
    "            if category in self.cat.values():\n",
    "                self.meta[self.id2cat[category]].append((os.path.join(self.root, category, 'points', uuid+'.pts'),\n",
    "                                        os.path.join(self.root, category, 'points_label', uuid+'.seg')))\n",
    "\n",
    "        self.datapath = []\n",
    "        for item in self.cat:\n",
    "            for fn in self.meta[item]:\n",
    "                self.datapath.append((item, fn[0], fn[1]))\n",
    "\n",
    "        self.classes = dict(zip(sorted(self.cat), range(len(self.cat))))\n",
    "        print(self.classes)\n",
    "        with open('./misc/num_seg_classes.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                ls = line.strip().split()\n",
    "                self.seg_classes[ls[0]] = int(ls[1])\n",
    "        self.num_seg_classes = self.seg_classes[list(self.cat.keys())[0]]\n",
    "        print(self.seg_classes, self.num_seg_classes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn = self.datapath[index]\n",
    "        cls = self.classes[self.datapath[index][0]]\n",
    "        point_set = np.loadtxt(fn[1]).astype(np.float32)\n",
    "        seg = np.loadtxt(fn[2]).astype(np.int64)\n",
    "        #print(point_set.shape, seg.shape)\n",
    "\n",
    "        choice = np.random.choice(len(seg), self.npoints, replace=True)\n",
    "        #resample\n",
    "        point_set = point_set[choice, :]\n",
    "\n",
    "        point_set = point_set - np.expand_dims(np.mean(point_set, axis = 0), 0) # center\n",
    "        dist = np.max(np.sqrt(np.sum(point_set ** 2, axis = 1)),0)\n",
    "        point_set = point_set / dist #scale\n",
    "\n",
    "        if self.data_augmentation:\n",
    "            theta = np.random.uniform(0,np.pi*2)\n",
    "            rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],[np.sin(theta), np.cos(theta)]])\n",
    "            point_set[:,[0,2]] = point_set[:,[0,2]].dot(rotation_matrix) # random rotation\n",
    "            point_set += np.random.normal(0, 0.02, size=point_set.shape) # random jitter\n",
    "\n",
    "        seg = seg[choice]\n",
    "        point_set = torch.from_numpy(point_set)\n",
    "        seg = torch.from_numpy(seg)\n",
    "        cls = torch.from_numpy(np.array([cls]).astype(np.int64))\n",
    "\n",
    "        if self.classification:\n",
    "            return point_set, cls\n",
    "        else:\n",
    "            return point_set, seg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapath)\n",
    "\n",
    "class ModelNetDataset(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 npoints=2500,\n",
    "                 split='train',\n",
    "                 data_augmentation=True):\n",
    "        self.npoints = npoints\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.fns = []\n",
    "        with open(os.path.join(root, '{}.txt'.format(self.split)), 'r') as f:\n",
    "            for line in f:\n",
    "                self.fns.append(line.strip())\n",
    "\n",
    "        self.cat = {}\n",
    "        with open('./misc/modelnet_id.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                ls = line.strip().split()\n",
    "                self.cat[ls[0]] = int(ls[1])\n",
    "\n",
    "        print(self.cat)\n",
    "        self.classes = list(self.cat.keys())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn = self.fns[index]\n",
    "        cls = self.cat[fn.split('/')[0]]\n",
    "        with open(os.path.join(self.root, fn), 'rb') as f:\n",
    "            plydata = PlyData.read(f)\n",
    "        pts = np.vstack([plydata['vertex']['x'], plydata['vertex']['y'], plydata['vertex']['z']]).T\n",
    "        choice = np.random.choice(len(pts), self.npoints, replace=True)\n",
    "        point_set = pts[choice, :]\n",
    "\n",
    "        point_set = point_set - np.expand_dims(np.mean(point_set, axis=0), 0)  # center\n",
    "        dist = np.max(np.sqrt(np.sum(point_set ** 2, axis=1)), 0)\n",
    "        point_set = point_set / dist  # scale\n",
    "\n",
    "        if self.data_augmentation:\n",
    "            theta = np.random.uniform(0, np.pi * 2)\n",
    "            rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "            point_set[:, [0, 2]] = point_set[:, [0, 2]].dot(rotation_matrix)  # random rotation\n",
    "            point_set += np.random.normal(0, 0.02, size=point_set.shape)  # random jitter\n",
    "\n",
    "        point_set = torch.from_numpy(point_set.astype(np.float32))\n",
    "        cls = torch.from_numpy(np.array([cls]).astype(np.int64))\n",
    "        return point_set, cls\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = sys.argv[1]\n",
    "    datapath = sys.argv[2]\n",
    "\n",
    "    if dataset == 'shapenet':\n",
    "        d = ShapeNetDataset(root = datapath, class_choice = ['Chair'])\n",
    "        print(len(d))\n",
    "        ps, seg = d[0]\n",
    "        print(ps.size(), ps.type(), seg.size(),seg.type())\n",
    "\n",
    "        d = ShapeNetDataset(root = datapath, classification = True)\n",
    "        print(len(d))\n",
    "        ps, cls = d[0]\n",
    "        print(ps.size(), ps.type(), cls.size(),cls.type())\n",
    "        # get_segmentation_classes(datapath)\n",
    "\n",
    "    if dataset == 'modelnet':\n",
    "        gen_modelnet_id(datapath)\n",
    "        d = ModelNetDataset(root=datapath)\n",
    "        print(len(d))\n",
    "        print(d[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "Me65ztfUn3Gk",
    "outputId": "7f3d4132-e8cf-49d1-c1b7-11a2006673c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.opt'>\n",
      "Random Seed:  4501\n",
      "{'Airplane': 0, 'Bag': 1, 'Cap': 2, 'Car': 3, 'Chair': 4, 'Earphone': 5, 'Guitar': 6, 'Knife': 7, 'Lamp': 8, 'Laptop': 9, 'Motorbike': 10, 'Mug': 11, 'Pistol': 12, 'Rocket': 13, 'Skateboard': 14, 'Table': 15}\n",
      "{'Airplane': 4, 'Bag': 2, 'Cap': 2, 'Car': 4, 'Chair': 4, 'Earphone': 3, 'Guitar': 3, 'Knife': 2, 'Lamp': 4, 'Laptop': 2, 'Motorbike': 6, 'Mug': 2, 'Pistol': 3, 'Rocket': 3, 'Skateboard': 3, 'Table': 3} 4\n",
      "{'Airplane': 0, 'Bag': 1, 'Cap': 2, 'Car': 3, 'Chair': 4, 'Earphone': 5, 'Guitar': 6, 'Knife': 7, 'Lamp': 8, 'Laptop': 9, 'Motorbike': 10, 'Mug': 11, 'Pistol': 12, 'Rocket': 13, 'Skateboard': 14, 'Table': 15}\n",
      "{'Airplane': 4, 'Bag': 2, 'Cap': 2, 'Car': 4, 'Chair': 4, 'Earphone': 3, 'Guitar': 3, 'Knife': 2, 'Lamp': 4, 'Laptop': 2, 'Motorbike': 6, 'Mug': 2, 'Pistol': 3, 'Rocket': 3, 'Skateboard': 3, 'Table': 3} 4\n",
      "12137 2874\n",
      "classes 16\n",
      "debug1\n",
      "debug2\n",
      "[0: 0/379] train loss: 2.935005 accuracy: 0.062500\n",
      "[0: 0/379] \u001b[94mtest\u001b[0m loss: 2.757623 accuracy: 0.250000\n",
      "[0: 1/379] train loss: 2.919477 accuracy: 0.062500\n",
      "[0: 2/379] train loss: 2.677499 accuracy: 0.187500\n",
      "[0: 3/379] train loss: 2.605810 accuracy: 0.250000\n",
      "[0: 4/379] train loss: 2.528243 accuracy: 0.312500\n",
      "[0: 5/379] train loss: 2.048291 accuracy: 0.531250\n",
      "[0: 6/379] train loss: 2.110638 accuracy: 0.406250\n",
      "[0: 7/379] train loss: 1.782369 accuracy: 0.687500\n",
      "[0: 8/379] train loss: 1.804962 accuracy: 0.625000\n",
      "[0: 9/379] train loss: 1.567938 accuracy: 0.687500\n",
      "[0: 10/379] train loss: 1.321016 accuracy: 0.718750\n",
      "[0: 10/379] \u001b[94mtest\u001b[0m loss: 2.576826 accuracy: 0.312500\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-261619315c6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;31m#ipdb.set_trace(context=5)#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-8aeccebe1236>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-929f9aadc95d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_transform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mtrans_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-8d0c51bc3b0f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 314.00 MiB (GPU 0; 14.76 GiB total capacity; 13.25 GiB already allocated; 145.75 MiB free; 13.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "#from pointnet.dataset import ShapeNetDataset, ModelNetDataset\n",
    "#from pointnet.model import PointNetCls, feature_transform_regularizer\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "#ipdb.set_trace(context=5)\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\n",
    "#    '--batchSize', type=int, default=32, help='input batch size')\n",
    "#parser.add_argument(\n",
    "#    '--num_points', type=int, default=2500, help='input batch size')\n",
    "#parser.add_argument(\n",
    "#    '--workers', type=int, help='number of data loading workers', default=4)\n",
    "#parser.add_argument(\n",
    "#    '--nepoch', type=int, default=250, help='number of epochs to train for')\n",
    "#parser.add_argument('--outf', type=str, default='cls', help='output folder')\n",
    "#parser.add_argument('--model', type=str, default='', help='model path')\n",
    "#parser.add_argument('--dataset', type=str, required=True, help=\"dataset path\")\n",
    "#parser.add_argument('--dataset_type', type=str, default='shapenet', help=\"dataset type shapenet|modelnet40\")\n",
    "#parser.add_argument('--feature_transform', action='store_true', help=\"use feature transform\")\n",
    "\n",
    "#ipdb.set_trace(context=5)\n",
    "#parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "#opt = parser.parse_args()\n",
    "\n",
    "class opt:\n",
    "  pass\n",
    "\n",
    "opt.batchSize=32\n",
    "opt.num_points=2500\n",
    "opt.workers=4\n",
    "opt.nepoch=250\n",
    "opt.outf='cls'\n",
    "opt.model=''\n",
    "opt.dataset='./shapenetcore_partanno_segmentation_benchmark_v0'\n",
    "opt.dataset_type='shapenet'\n",
    "opt.feature_transform='store_true'\n",
    "\n",
    "\n",
    "print(opt)\n",
    "\n",
    "blue = lambda x: '\\033[94m' + x + '\\033[0m'\n",
    "\n",
    "opt.manualSeed = random.randint(1, 10000)  # fix seed\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "if opt.dataset_type == 'shapenet':\n",
    "    dataset = ShapeNetDataset(\n",
    "        root=opt.dataset,\n",
    "        classification=True,\n",
    "        npoints=opt.num_points)\n",
    "\n",
    "    test_dataset = ShapeNetDataset(\n",
    "        root=opt.dataset,\n",
    "        classification=True,\n",
    "        split='test',\n",
    "        npoints=opt.num_points,\n",
    "        data_augmentation=False)\n",
    "elif opt.dataset_type == 'modelnet40':\n",
    "    dataset = ModelNetDataset(\n",
    "        root=opt.dataset,\n",
    "        npoints=opt.num_points,\n",
    "        split='train_files')\n",
    "\n",
    "    test_dataset = ModelNetDataset(\n",
    "        root=opt.dataset,\n",
    "        split='test_files',\n",
    "        npoints=opt.num_points,\n",
    "        data_augmentation=False)\n",
    "else:\n",
    "    exit('wrong dataset type')\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=opt.batchSize,\n",
    "    shuffle=True,\n",
    "    num_workers=int(opt.workers))\n",
    "\n",
    "testdataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=opt.batchSize,\n",
    "        shuffle=True,\n",
    "        num_workers=int(opt.workers))\n",
    "\n",
    "print(len(dataset), len(test_dataset))\n",
    "num_classes = len(dataset.classes)\n",
    "print('classes', num_classes)\n",
    "\n",
    "try:\n",
    "    os.makedirs(opt.outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "classifier = PointNetCls(k=num_classes, feature_transform=opt.feature_transform)\n",
    "\n",
    "if opt.model != '':\n",
    "    classifier.load_state_dict(torch.load(opt.model))\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "classifier.cuda()\n",
    "\n",
    "num_batch = len(dataset) / opt.batchSize\n",
    "\n",
    "for epoch in range(opt.nepoch):\n",
    "    scheduler.step()\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        #ipdb.set_trace(context=5)#\n",
    "        points, target = data\n",
    "        target = target[:, 0]\n",
    "        points = points.transpose(2, 1)\n",
    "        points, target = points.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        classifier = classifier.train()\n",
    "        pred, trans, trans_feat = classifier(points)\n",
    "        #ipdb.set_trace(context=5)#\n",
    "        loss = F.nll_loss(pred, target)\n",
    "        if opt.feature_transform:\n",
    "            loss += feature_transform_regularizer(trans_feat) * 0.001\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "        correct = pred_choice.eq(target.data).cpu().sum()\n",
    "        print('[%d: %d/%d] train loss: %f accuracy: %f' % (epoch, i, num_batch, loss.item(), correct.item() / float(opt.batchSize)))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            j, data = next(enumerate(testdataloader, 0))\n",
    "            points, target = data\n",
    "            target = target[:, 0]\n",
    "            points = points.transpose(2, 1)\n",
    "            points, target = points.cuda(), target.cuda()\n",
    "            classifier = classifier.eval()\n",
    "            pred, _, _ = classifier(points)\n",
    "            loss = F.nll_loss(pred, target)\n",
    "            pred_choice = pred.data.max(1)[1]\n",
    "            correct = pred_choice.eq(target.data).cpu().sum()\n",
    "            print('[%d: %d/%d] %s loss: %f accuracy: %f' % (epoch, i, num_batch, blue('test'), loss.item(), correct.item()/float(opt.batchSize)))\n",
    "\n",
    "    torch.save(classifier.state_dict(), '%s/cls_model_%d.pth' % (opt.outf, epoch))\n",
    "\n",
    "total_correct = 0\n",
    "total_testset = 0\n",
    "for i,data in tqdm(enumerate(testdataloader, 0)):\n",
    "    points, target = data\n",
    "    target = target[:, 0]\n",
    "    points = points.transpose(2, 1)\n",
    "    points, target = points.cuda(), target.cuda()\n",
    "    classifier = classifier.eval()\n",
    "    pred, _, _ = classifier(points)\n",
    "    pred_choice = pred.data.max(1)[1]\n",
    "    correct = pred_choice.eq(target.data).cpu().sum()\n",
    "    total_correct += correct.item()\n",
    "    total_testset += points.size()[0]\n",
    "\n",
    "print(\"final accuracy {}\".format(total_correct / float(total_testset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGtjjY5xNV1F",
    "outputId": "fd5de0fe-452e-4abf-c363-03eb07593a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 13 11:33:06 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   68C    P0    31W /  70W |  14964MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AyT2T2o8st4F",
    "outputId": "d67cb8fb-ca15-4d0a-c5d7-e0d061ac4b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sofa_0646.off\n"
     ]
    }
   ],
   "source": [
    "!ls ./ModelNet40/sofa/train | grep sofa_0646.off"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#PYTHON INSTALACJA"
      ],
      "metadata": {
        "id": "WH_uACqHFbMT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS5b819tJ3WX",
        "outputId": "e614c1f8-3378-45ed-8bf3-d24f540ae100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 153 kB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 117 kB 50.0 MB/s \n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for unzip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 761 kB 4.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 386 kB 64.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 54.2 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 8.7.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "#pip install pytorch-lightning --quiet\n",
        "#pip install wandb --quiet\n",
        "pip install hydra-core --upgrade --quiet\n",
        "pip install unzip --quiet\n",
        "pip install ipdb -Uqq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PYTHON PAKIETY"
      ],
      "metadata": {
        "id": "aOmG3gHqFd4r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "V3SM_OA7JPkr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fcb4e603-485c-4018-eab8-0dbed3d82a6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Weights and Biases\\nimport wandb\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# standardowe pakiety\n",
        "import os\n",
        "from os.path import join, isfile\n",
        "from os import listdir\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import scipy as sp\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import random\n",
        "import glob\n",
        "import ipdb\n",
        "from __future__ import print_function\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Pytorch \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.utils.data as data\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Compose, ToPILImage, Normalize, Resize, ToTensor\n",
        "from torch.autograd import Variable\n",
        "'''\n",
        "# Pytorch Lightning related imports\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "import torchmetrics\n",
        "'''\n",
        "# Hydra\n",
        "import hydra\n",
        "from hydra.utils import instantiate\n",
        "from omegaconf import DictConfig, OmegaConf\n",
        "'''\n",
        "# Weights and Biases\n",
        "import wandb\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#POBIERANIE DANYCH"
      ],
      "metadata": {
        "id": "5TH9ZK2tFkB8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIh7Q2ZoO9F3",
        "outputId": "42e64af3-a702-4eff-865a-71739c5ee3a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "%%shell\n",
        "wget http://modelnet.cs.princeton.edu/ModelNet40.zip\n",
        "mkdir /content/model40\n",
        "unzip -qq /content/ModelNet40.zip -d /content/model40"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "wget http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n",
        "mkdir /content/model10\n",
        "unzip -qq /content/ModelNet10.zip -d /content/model10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVIUHEhAlKsV",
        "outputId": "18dae0bd-1ce5-4eed-fcbf-4a3d69774bb5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrbLLONOZkh3"
      },
      "source": [
        "#MODELNET LOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "onC4Y1QCZkIK"
      },
      "outputs": [],
      "source": [
        "class ModelNetLoader(data.Dataset):\n",
        "    def __init__(self, root, split='train', num_classes=40, numpoints=5000, batch_size=32, transform=None, target_transform=None):\n",
        "        self.root = root\n",
        "        self.numpoints = numpoints\n",
        "        self.num_classes = num_classes\n",
        "        self.batch_size = batch_size\n",
        "        self.filelist = [f1 for f1 in glob.glob(self.root + '**/*.off', recursive=True)]\n",
        "        self.trainlist = [f2 for f2 in self.filelist if \"train\" in f2]\n",
        "        self.testlist = [f3 for f3 in self.filelist if \"test\" in f3]\n",
        "        if split=='train': self.objlist = self.flist_reader(self.trainlist)\n",
        "        if split=='test': self.objlist = self.flist_reader(self.testlist)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        impath, target = self.objlist[index]\n",
        "        myF = open(impath, 'r')\n",
        "        firstLine = myF.readline().strip()\n",
        "        if 'OFF' != firstLine[0:3]:\n",
        "            raise ValueError('Not a valid OFF header')\n",
        "        if len(firstLine) > 3:\n",
        "            n_verts, n_faces, n_edges = tuple([int(s) for s in firstLine[3:].split(' ')])\n",
        "        else:\n",
        "            n_verts, n_faces, n_edges = tuple([int(s) for s in myF.readline().strip().split(' ')])\n",
        "        verts = np.array([[float(s) for s in myF.readline().strip().split(' ')] for i_vert in range(n_verts)])\n",
        "        choice = np.random.choice(verts.shape[0], self.numpoints, replace=True)\n",
        "        point_set = verts[choice, :]\n",
        "        #faces = [[int(s) for s in myF.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n",
        "        point_set = point_set - np.expand_dims(np.mean(point_set, axis=0), 0) \n",
        "        dist = np.max(np.sqrt(np.sum(point_set ** 2, axis=1)), 0)\n",
        "        point_set = point_set / dist  \n",
        "        #poniższe dwie linijki nowe\n",
        "        point_set = torch.from_numpy(point_set.astype(np.float32)).float()\n",
        "        #target = torch.from_numpy(np.array([target]).astype(np.int64))\n",
        "        return point_set, target\n",
        "\n",
        "    def flist_reader(self, filelist):\n",
        "        classes_dirs = glob.glob(self.root + \"*/\", recursive = False)\n",
        "        classes_list = [i.split('/')[4] for i in classes_dirs]\n",
        "        classes_nums = np.linspace(0,self.num_classes-1,self.num_classes)\n",
        "        classes_dict = dict(zip(sorted(classes_list), classes_nums.astype(int)))\n",
        "        objlist = []\n",
        "\n",
        "        for line in filelist:\n",
        "            label_start_index = line.strip().find(self.root) + len(self.root)\n",
        "            label_end_index = line.strip()[label_start_index:].find('/')\n",
        "            imlabel = line.strip()[label_start_index:label_start_index+label_end_index]\n",
        "            imlabel = classes_dict[imlabel]\n",
        "            impath = line\n",
        "            objlist.append( (impath, imlabel) )\n",
        "        return objlist\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.objlist)\n",
        "\n",
        "    #def train_dataloader(self):\n",
        "    #    return DataLoader(dataset=self, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=self.shuf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TEST FOR SINGLE OBJECT"
      ],
      "metadata": {
        "id": "_IRwAlmPKgsr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Z6MoIQvBNh7m"
      },
      "outputs": [],
      "source": [
        "def disp(tensor_image, target):\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = plt.axes(projection='3d')\n",
        "    V_np = np.asarray(tensor_image.cpu())\n",
        "    xx = V_np[:, 0]\n",
        "    yy = V_np[:, 1]\n",
        "    zz = V_np[:, 2]\n",
        "    ax.scatter(xx, yy, zz)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    print('Etykieta klasy: {}'.format(target))\n",
        "    print()\n",
        "\n",
        "mdltest = ModelNetLoader(root='/content/model40/ModelNet40/')\n",
        "\n",
        "#test for all objects\n",
        "'''\n",
        "print(len(mdltest))\n",
        "\n",
        "for n in range(len(mdltest)):\n",
        "  xxx = mdltest[n]\n",
        "  if xxx[0].shape[0] == 0:\n",
        "    ipdb.set_trace(context=5)\n",
        "'''\n",
        "\n",
        "xxx = mdltest[random.randint(0, len(mdltest))]\n",
        "\n",
        "disp(xxx[0], xxx[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yvJCbVGZmgL"
      },
      "source": [
        "#MODELNET DATA MODULE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtPIaxNTNhZJ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class ModelNetDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size, npoints=100, nworkers=1, data_dir: str = './model10/ModelNet10/'):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = 10\n",
        "        self.npoints = npoints\n",
        "        self.nworkers = nworkers\n",
        "        self.imglist = [f1 for f1 in glob.glob(data_dir + '**/*.off', recursive=True)]# if isfile(join(self.data_dir, f1))]\n",
        "        self.trainlist = [f2 for f2 in self.imglist if \"train\" in f2]\n",
        "        self.testlist = [f3 for f3 in self.imglist if \"test\" in f3]\n",
        "\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        #training / validation\n",
        "        if stage == 'fit' or stage is None:\n",
        "            train_dataset = ModelNetFileList(root=self.data_dir, numpoints=self.npoints, flist=self.trainlist)\n",
        "            train_dataset_size = int(len(train_dataset) * 0.9)\n",
        "            self.train_dataset, self.val_dataset = random_split(train_dataset, [train_dataset_size, len(train_dataset) - train_dataset_size])\n",
        "        #testing\n",
        "        if stage == 'test' or stage is None:\n",
        "            self.test_dataset = ModelNetFileList(root=self.data_dir, numpoints=self.npoints, flist=self.testlist)\n",
        "\n",
        "    def prepare_batch(self):\n",
        "        batch_tensor = torch.empty((self.batch_size, 3, self.npoints))\n",
        "        for n in range(self.batch_size):\n",
        "            obj, target = random.choice(self.train_dataset) \n",
        "            obj = obj.view(obj.shape[1], obj.shape[0])\n",
        "            batch_tensor[n, :, :] = obj\n",
        "        self.batch_tensor = batch_tensor\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.nworkers, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.nworkers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.nworkers)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelNetDataset(data.Dataset):\n",
        "    def __init__(self,\n",
        "                 root,\n",
        "                 npoints=2500,\n",
        "                 split='train',\n",
        "                 data_augmentation=True):\n",
        "        self.npoints = npoints\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.fns = []\n",
        "        with open(os.path.join(root, '{}.txt'.format(self.split)), 'r') as f:\n",
        "            for line in f:\n",
        "                self.fns.append(line.strip())\n",
        "\n",
        "        self.cat = {}\n",
        "        with open('./misc/modelnet_id.txt', 'r') as f:\n",
        "            for line in f:\n",
        "                ls = line.strip().split()\n",
        "                self.cat[ls[0]] = int(ls[1])\n",
        "\n",
        "        print(self.cat)\n",
        "        self.classes = list(self.cat.keys())\n",
        "    '''\n",
        "    def __getitem2__(self, index):\n",
        "      #impath, target = self.imlist[index]\n",
        "      myF = open(os.path.join(self.root, '{}.txt'.format(self.split)), 'r')\n",
        "      firstLine = myF.readline().strip()\n",
        "      if 'OFF' != firstLine[0:3]:\n",
        "          raise ValueError('Not a valid OFF header')\n",
        "      if len(firstLine) > 3:\n",
        "          n_verts, n_faces, n_edges = tuple([int(s) for s in firstLine[4:].split(' ')])\n",
        "      else:\n",
        "          n_verts, n_faces, n_edges = tuple([int(s) for s in myF.readline().strip().split(' ')])\n",
        "      verts = torch.tensor([[float(s) for s in myF.readline().strip().split(' ')] for i_vert in range(n_verts)])\n",
        "      #ipdb.set_trace(context=5)#\n",
        "      if verts.shape[0] > 0:\n",
        "        choice = np.random.choice(verts.shape[0], numpoints, replace=True)\n",
        "        point_set = verts[choice, :]\n",
        "      else:\n",
        "        point_set=verts \n",
        "      #faces = [[int(s) for s in myF.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n",
        "      return point_set, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fns)\n",
        "    '''\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ###ipdb.set_trace(context=5)#\n",
        "        fn = self.fns[index]\n",
        "        cls = self.cat[fn.split('/')[0]]\n",
        "        #ipdb.set_trace(context=5)#\n",
        "\n",
        "        '''\n",
        "        with open(os.path.join(self.root, fn), 'rb') as f:\n",
        "            plydata = PlyData.read(f)\n",
        "        pts = np.vstack([plydata['vertex']['x'], plydata['vertex']['y'], plydata['vertex']['z']]).T\n",
        "        \n",
        "        '''\n",
        "        ipdb.set_trace(context=5)\n",
        "        with open(os.path.join(self.root, fn), 'r') as myF:\n",
        "            firstLine = myF.readline().strip()\n",
        "            #if 'OFF' != firstLine[0:3]:\n",
        "            #    raise ValueError('Not a valid OFF header')\n",
        "            if len(firstLine) > 3:\n",
        "                n_verts, n_faces, n_edges = tuple([int(s) for s in firstLine[4:].split(' ')])\n",
        "            else:\n",
        "                n_verts, n_faces, n_edges = tuple([int(s) for s in myF.readline().strip().split(' ')])\n",
        "            pts = torch.tensor([[float(s) for s in myF.readline().strip().split(' ')] for i_vert in range(n_verts)])       \n",
        "\n",
        "        \n",
        "\n",
        "        choice = np.random.choice(len(pts), self.npoints, replace=True)\n",
        "        point_set = pts[choice, :]\n",
        "\n",
        "        point_set = point_set - np.expand_dims(np.mean(point_set, axis=0), 0)  # center\n",
        "        dist = np.max(np.sqrt(np.sum(point_set ** 2, axis=1)), 0)\n",
        "        point_set = point_set / dist  # scale\n",
        "\n",
        "        if self.data_augmentation:\n",
        "            theta = np.random.uniform(0, np.pi * 2)\n",
        "            rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "            point_set[:, [0, 2]] = point_set[:, [0, 2]].dot(rotation_matrix)  # random rotation\n",
        "            point_set += np.random.normal(0, 0.02, size=point_set.shape)  # random jitter\n",
        "\n",
        "        point_set = torch.from_numpy(point_set.astype(np.float32))\n",
        "        cls = torch.from_numpy(np.array([cls]).astype(np.int64))\n",
        "        return point_set, cls        "
      ],
      "metadata": {
        "id": "2tNUBhSBqLuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RndcfcMaQg1c"
      },
      "source": [
        "#STN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class STN(nn.Module):\n",
        "    def __init__(self, kd=True):\n",
        "        if kd:  self.k=64; self.iden_arr=np.eye(self.k).flatten()\n",
        "        else:   self.k=3;  self.iden_arr=np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]) \n",
        "        super(STN, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv1d(self.k, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, self.k*self.k)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.bns = []\n",
        "        batch_norm_args = np.logspace(6, 10, base=2, num=5, dtype=np.int64)\n",
        "        for n in batch_norm_args: self.bns.append(nn.BatchNorm1d(n)) \n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.shape[0]\n",
        "        x = F.relu(self.bns[0](self.conv1(x)))\n",
        "        x = F.relu(self.bns[1](self.conv2(x)))\n",
        "        x = F.relu(self.bns[4](self.conv3(x)))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024) \n",
        "        x = F.relu(self.bns[3](self.fc1(x))) \n",
        "        x = F.relu(self.bns[2](self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        iden = Variable(torch.from_numpy(self.iden_arr.astype(np.float32))).view(1, self.k*self.k).repeat(batchsize, 1) #stare, zmienić\n",
        "\n",
        "        if x.is_cuda:\n",
        "            iden = iden.cuda()#pytorch lightning, tako rzeczye Gwardys\n",
        "        x = x + iden\n",
        "        x = x.view(-1, self.k, self.k)\n",
        "        return x"
      ],
      "metadata": {
        "id": "BEG3IWrJrRX-"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTp4RkRhJDmq"
      },
      "source": [
        "#POINTNET FEATURE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "-5uBvfk3JCvO"
      },
      "outputs": [],
      "source": [
        "class PointNetfeat(nn.Module):\n",
        "    def __init__(self, global_feat = True, feature_transform = False):\n",
        "        super(PointNetfeat, self).__init__()\n",
        "        self.stn = STN(kd=False)\n",
        "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.global_feat = global_feat\n",
        "        self.feature_transform = feature_transform\n",
        "        if self.feature_transform:\n",
        "            self.fstn = STN()\n",
        "\n",
        "    def forward(self, x):\n",
        "        n_pts = x.shape[2]\n",
        "        trans = self.stn(x)\n",
        "        #ipdb.set_trace(context=5)\n",
        "        x = x.transpose(2, 1)\n",
        "        x = torch.bmm(x, trans)\n",
        "        x = x.transpose(2, 1)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        if self.feature_transform:\n",
        "            trans_feat = self.fstn(x)\n",
        "            x = x.transpose(2,1)\n",
        "            x = torch.bmm(x, trans_feat)\n",
        "            x = x.transpose(2,1)\n",
        "        else:\n",
        "            trans_feat = None\n",
        "\n",
        "        pointfeat = x\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "        if self.global_feat:\n",
        "            return x, trans, trans_feat\n",
        "        else:\n",
        "            x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
        "            return torch.cat([x, pointfeat], 1), trans, trans_feat\n",
        "\n",
        "\n",
        "def feature_transform_regularizer(trans):\n",
        "    d = trans.size()[1]\n",
        "    batchsize = trans.size()[0]\n",
        "    I = torch.eye(d)[None, :, :]\n",
        "    if trans.is_cuda:\n",
        "        I = I.cuda()\n",
        "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2)))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJe4TJxESIeg"
      },
      "source": [
        "#POINTNET CLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "iNImn71YSBBn"
      },
      "outputs": [],
      "source": [
        "class PointNetCls(nn.Module):\n",
        "    def __init__(self, k=2, feature_transform=False):\n",
        "        super(PointNetCls, self).__init__()\n",
        "        self.feature_transform = feature_transform\n",
        "        self.feat = PointNetfeat(global_feat=True, feature_transform=feature_transform)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, k)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, trans, trans_feat = self.feat(x)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = F.relu(self.bn2(self.dropout(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1), trans, trans_feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5cpfHkxWTQt",
        "outputId": "f5d7277f-f9f6-4874-8fad-dfb2d7499618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!nvidia-smi -L  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bz8yEL5VZP9"
      },
      "source": [
        "#TRENOWANIE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QVkQgB__VYML",
        "outputId": "230412cb-87e1-4638-e0a1-a38acfc63be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:  6300\n",
            "Rozmiar zbioru treningowego: 3591\n",
            "Rozmiar zbioru walidacyjnego: 400\n",
            "Rozmiar zbioru testowego: 908\n",
            "Rozmiar batcha: 32\n",
            "Liczba klas: 10\n",
            "Epoch 0: batch 0 of 112. Train loss: 2.588857 Accuracy: 0.031250\n",
            "Epoch 0: batch 0 of 112. \u001b[94mValidation\u001b[0m loss: 10.307782 Accuracy: 0.218750\n",
            "Epoch 0: batch 1 of 112. Train loss: 2.756212 Accuracy: 0.187500\n",
            "Epoch 0: batch 2 of 112. Train loss: 3.450259 Accuracy: 0.343750\n",
            "Epoch 0: batch 3 of 112. Train loss: 3.506100 Accuracy: 0.187500\n",
            "Epoch 0: batch 4 of 112. Train loss: 3.387805 Accuracy: 0.125000\n",
            "Epoch 0: batch 5 of 112. Train loss: 3.121886 Accuracy: 0.312500\n",
            "Epoch 0: batch 6 of 112. Train loss: 2.617417 Accuracy: 0.312500\n",
            "Epoch 0: batch 7 of 112. Train loss: 2.320624 Accuracy: 0.375000\n",
            "Epoch 0: batch 8 of 112. Train loss: 2.262897 Accuracy: 0.312500\n",
            "Epoch 0: batch 9 of 112. Train loss: 1.974464 Accuracy: 0.562500\n",
            "Epoch 0: batch 10 of 112. Train loss: 1.996893 Accuracy: 0.312500\n",
            "Epoch 0: batch 10 of 112. \u001b[94mValidation\u001b[0m loss: 1.977826 Accuracy: 0.375000\n",
            "Epoch 0: batch 11 of 112. Train loss: 1.885596 Accuracy: 0.375000\n",
            "Epoch 0: batch 12 of 112. Train loss: 2.457704 Accuracy: 0.343750\n",
            "Epoch 0: batch 13 of 112. Train loss: 2.100546 Accuracy: 0.343750\n",
            "Epoch 0: batch 14 of 112. Train loss: 1.822698 Accuracy: 0.500000\n",
            "Epoch 0: batch 15 of 112. Train loss: 2.431710 Accuracy: 0.343750\n",
            "Epoch 0: batch 16 of 112. Train loss: 2.131916 Accuracy: 0.437500\n",
            "Epoch 0: batch 17 of 112. Train loss: 2.117602 Accuracy: 0.312500\n",
            "Epoch 0: batch 18 of 112. Train loss: 1.999596 Accuracy: 0.406250\n",
            "Epoch 0: batch 19 of 112. Train loss: 1.992128 Accuracy: 0.468750\n",
            "Epoch 0: batch 20 of 112. Train loss: 1.936008 Accuracy: 0.375000\n",
            "Epoch 0: batch 20 of 112. \u001b[94mValidation\u001b[0m loss: 2.487671 Accuracy: 0.437500\n",
            "Epoch 0: batch 21 of 112. Train loss: 1.708346 Accuracy: 0.375000\n",
            "Epoch 0: batch 22 of 112. Train loss: 1.966338 Accuracy: 0.406250\n",
            "Epoch 0: batch 23 of 112. Train loss: 1.843585 Accuracy: 0.500000\n",
            "Epoch 0: batch 24 of 112. Train loss: 1.577067 Accuracy: 0.531250\n",
            "Epoch 0: batch 25 of 112. Train loss: 1.717117 Accuracy: 0.531250\n",
            "Epoch 0: batch 26 of 112. Train loss: 1.832109 Accuracy: 0.437500\n",
            "Epoch 0: batch 27 of 112. Train loss: 1.896641 Accuracy: 0.343750\n",
            "Epoch 0: batch 28 of 112. Train loss: 1.946092 Accuracy: 0.406250\n",
            "Epoch 0: batch 29 of 112. Train loss: 1.642939 Accuracy: 0.468750\n",
            "Epoch 0: batch 30 of 112. Train loss: 1.500433 Accuracy: 0.687500\n",
            "Epoch 0: batch 30 of 112. \u001b[94mValidation\u001b[0m loss: 2.085168 Accuracy: 0.500000\n",
            "Epoch 0: batch 31 of 112. Train loss: 1.606102 Accuracy: 0.625000\n",
            "Epoch 0: batch 32 of 112. Train loss: 1.929661 Accuracy: 0.468750\n",
            "Epoch 0: batch 33 of 112. Train loss: 1.450486 Accuracy: 0.625000\n",
            "Epoch 0: batch 34 of 112. Train loss: 2.054635 Accuracy: 0.343750\n",
            "Epoch 0: batch 35 of 112. Train loss: 1.382914 Accuracy: 0.687500\n",
            "Epoch 0: batch 36 of 112. Train loss: 1.867618 Accuracy: 0.406250\n",
            "Epoch 0: batch 37 of 112. Train loss: 1.722894 Accuracy: 0.468750\n",
            "Epoch 0: batch 38 of 112. Train loss: 2.244931 Accuracy: 0.437500\n",
            "Epoch 0: batch 39 of 112. Train loss: 2.096082 Accuracy: 0.531250\n",
            "Epoch 0: batch 40 of 112. Train loss: 2.446178 Accuracy: 0.562500\n",
            "Epoch 0: batch 40 of 112. \u001b[94mValidation\u001b[0m loss: 1.922370 Accuracy: 0.500000\n",
            "Epoch 0: batch 41 of 112. Train loss: 2.208135 Accuracy: 0.468750\n",
            "Epoch 0: batch 42 of 112. Train loss: 1.998760 Accuracy: 0.375000\n",
            "Epoch 0: batch 43 of 112. Train loss: 2.267235 Accuracy: 0.406250\n",
            "Epoch 0: batch 44 of 112. Train loss: 2.389715 Accuracy: 0.281250\n",
            "Epoch 0: batch 45 of 112. Train loss: 1.736639 Accuracy: 0.500000\n",
            "Epoch 0: batch 46 of 112. Train loss: 1.839985 Accuracy: 0.500000\n",
            "Epoch 0: batch 47 of 112. Train loss: 1.733813 Accuracy: 0.531250\n",
            "Epoch 0: batch 48 of 112. Train loss: 1.773304 Accuracy: 0.468750\n",
            "Epoch 0: batch 49 of 112. Train loss: 1.720387 Accuracy: 0.625000\n",
            "Epoch 0: batch 50 of 112. Train loss: 1.693865 Accuracy: 0.531250\n",
            "Epoch 0: batch 50 of 112. \u001b[94mValidation\u001b[0m loss: 1.794262 Accuracy: 0.406250\n",
            "Epoch 0: batch 51 of 112. Train loss: 1.712650 Accuracy: 0.500000\n",
            "Epoch 0: batch 52 of 112. Train loss: 2.016615 Accuracy: 0.468750\n",
            "Epoch 0: batch 53 of 112. Train loss: 1.605354 Accuracy: 0.562500\n",
            "Epoch 0: batch 54 of 112. Train loss: 1.537701 Accuracy: 0.468750\n",
            "Epoch 0: batch 55 of 112. Train loss: 1.677736 Accuracy: 0.375000\n",
            "Epoch 0: batch 56 of 112. Train loss: 1.869648 Accuracy: 0.562500\n",
            "Epoch 0: batch 57 of 112. Train loss: 1.640655 Accuracy: 0.593750\n",
            "Epoch 0: batch 58 of 112. Train loss: 2.272445 Accuracy: 0.406250\n",
            "Epoch 0: batch 59 of 112. Train loss: 1.718845 Accuracy: 0.562500\n",
            "Epoch 0: batch 60 of 112. Train loss: 2.013701 Accuracy: 0.343750\n",
            "Epoch 0: batch 60 of 112. \u001b[94mValidation\u001b[0m loss: 1.023873 Accuracy: 0.781250\n",
            "Epoch 0: batch 61 of 112. Train loss: 1.574000 Accuracy: 0.531250\n",
            "Epoch 0: batch 62 of 112. Train loss: 2.026040 Accuracy: 0.406250\n",
            "Epoch 0: batch 63 of 112. Train loss: 1.841690 Accuracy: 0.500000\n",
            "Epoch 0: batch 64 of 112. Train loss: 1.661913 Accuracy: 0.500000\n",
            "Epoch 0: batch 65 of 112. Train loss: 1.695624 Accuracy: 0.406250\n",
            "Epoch 0: batch 66 of 112. Train loss: 1.407805 Accuracy: 0.593750\n",
            "Epoch 0: batch 67 of 112. Train loss: 1.427068 Accuracy: 0.625000\n",
            "Epoch 0: batch 68 of 112. Train loss: 1.853383 Accuracy: 0.468750\n",
            "Epoch 0: batch 69 of 112. Train loss: 1.693485 Accuracy: 0.500000\n",
            "Epoch 0: batch 70 of 112. Train loss: 1.488337 Accuracy: 0.531250\n",
            "Epoch 0: batch 70 of 112. \u001b[94mValidation\u001b[0m loss: 1.224390 Accuracy: 0.531250\n",
            "Epoch 0: batch 71 of 112. Train loss: 1.483856 Accuracy: 0.562500\n",
            "Epoch 0: batch 72 of 112. Train loss: 1.230224 Accuracy: 0.750000\n",
            "Epoch 0: batch 73 of 112. Train loss: 1.645011 Accuracy: 0.625000\n",
            "Epoch 0: batch 74 of 112. Train loss: 2.013689 Accuracy: 0.437500\n",
            "Epoch 0: batch 75 of 112. Train loss: 1.616908 Accuracy: 0.531250\n",
            "Epoch 0: batch 76 of 112. Train loss: 2.493079 Accuracy: 0.312500\n",
            "Epoch 0: batch 77 of 112. Train loss: 1.468679 Accuracy: 0.656250\n",
            "Epoch 0: batch 78 of 112. Train loss: 1.605808 Accuracy: 0.531250\n",
            "Epoch 0: batch 79 of 112. Train loss: 1.680734 Accuracy: 0.562500\n",
            "Epoch 0: batch 80 of 112. Train loss: 1.693871 Accuracy: 0.562500\n",
            "Epoch 0: batch 80 of 112. \u001b[94mValidation\u001b[0m loss: 1.291402 Accuracy: 0.468750\n",
            "Epoch 0: batch 81 of 112. Train loss: 1.586965 Accuracy: 0.500000\n",
            "Epoch 0: batch 82 of 112. Train loss: 1.673195 Accuracy: 0.531250\n",
            "Epoch 0: batch 83 of 112. Train loss: 1.922878 Accuracy: 0.531250\n",
            "Epoch 0: batch 84 of 112. Train loss: 1.590431 Accuracy: 0.562500\n",
            "Epoch 0: batch 85 of 112. Train loss: 1.625827 Accuracy: 0.562500\n",
            "Epoch 0: batch 86 of 112. Train loss: 1.379505 Accuracy: 0.500000\n",
            "Epoch 0: batch 87 of 112. Train loss: 1.768213 Accuracy: 0.437500\n",
            "Epoch 0: batch 88 of 112. Train loss: 1.678390 Accuracy: 0.531250\n",
            "Epoch 0: batch 89 of 112. Train loss: 1.700768 Accuracy: 0.531250\n",
            "Epoch 0: batch 90 of 112. Train loss: 1.896000 Accuracy: 0.500000\n",
            "Epoch 0: batch 90 of 112. \u001b[94mValidation\u001b[0m loss: 1.361685 Accuracy: 0.531250\n",
            "Epoch 0: batch 91 of 112. Train loss: 1.948507 Accuracy: 0.437500\n",
            "Epoch 0: batch 92 of 112. Train loss: 1.689989 Accuracy: 0.593750\n",
            "Epoch 0: batch 93 of 112. Train loss: 1.720223 Accuracy: 0.656250\n",
            "Epoch 0: batch 94 of 112. Train loss: 1.808498 Accuracy: 0.562500\n",
            "Epoch 0: batch 95 of 112. Train loss: 1.442045 Accuracy: 0.562500\n",
            "Epoch 0: batch 96 of 112. Train loss: 1.753784 Accuracy: 0.593750\n",
            "Epoch 0: batch 97 of 112. Train loss: 1.625983 Accuracy: 0.593750\n",
            "Epoch 0: batch 98 of 112. Train loss: 1.963649 Accuracy: 0.375000\n",
            "Epoch 0: batch 99 of 112. Train loss: 1.558511 Accuracy: 0.531250\n",
            "Epoch 0: batch 100 of 112. Train loss: 1.416122 Accuracy: 0.656250\n",
            "Epoch 0: batch 100 of 112. \u001b[94mValidation\u001b[0m loss: 0.942913 Accuracy: 0.781250\n",
            "Epoch 0: batch 101 of 112. Train loss: 1.500269 Accuracy: 0.625000\n",
            "Epoch 0: batch 102 of 112. Train loss: 1.421606 Accuracy: 0.562500\n",
            "Epoch 0: batch 103 of 112. Train loss: 1.244323 Accuracy: 0.625000\n",
            "Epoch 0: batch 104 of 112. Train loss: 1.186102 Accuracy: 0.687500\n",
            "Epoch 0: batch 105 of 112. Train loss: 1.912319 Accuracy: 0.437500\n",
            "Epoch 0: batch 106 of 112. Train loss: 1.419794 Accuracy: 0.593750\n",
            "Epoch 0: batch 107 of 112. Train loss: 1.897238 Accuracy: 0.375000\n",
            "Epoch 0: batch 108 of 112. Train loss: 1.416639 Accuracy: 0.593750\n",
            "Epoch 0: batch 109 of 112. Train loss: 1.108296 Accuracy: 0.687500\n",
            "Epoch 0: batch 110 of 112. Train loss: 1.427580 Accuracy: 0.437500\n",
            "Epoch 0: batch 110 of 112. \u001b[94mValidation\u001b[0m loss: 1.484901 Accuracy: 0.468750\n",
            "Epoch 0: batch 111 of 112. Train loss: 1.499794 Accuracy: 0.500000\n",
            "Epoch 0: batch 112 of 112. Train loss: 1.357559 Accuracy: 0.125000\n",
            "Epoch 1: batch 0 of 112. Train loss: 1.196149 Accuracy: 0.625000\n",
            "Epoch 1: batch 0 of 112. \u001b[94mValidation\u001b[0m loss: 1.160434 Accuracy: 0.625000\n",
            "Epoch 1: batch 1 of 112. Train loss: 1.229502 Accuracy: 0.593750\n",
            "Epoch 1: batch 2 of 112. Train loss: 1.397292 Accuracy: 0.625000\n",
            "Epoch 1: batch 3 of 112. Train loss: 1.720905 Accuracy: 0.406250\n",
            "Epoch 1: batch 4 of 112. Train loss: 1.153165 Accuracy: 0.687500\n",
            "Epoch 1: batch 5 of 112. Train loss: 1.438889 Accuracy: 0.625000\n",
            "Epoch 1: batch 6 of 112. Train loss: 1.597638 Accuracy: 0.437500\n",
            "Epoch 1: batch 7 of 112. Train loss: 1.529423 Accuracy: 0.531250\n",
            "Epoch 1: batch 8 of 112. Train loss: 1.589310 Accuracy: 0.531250\n",
            "Epoch 1: batch 9 of 112. Train loss: 1.115627 Accuracy: 0.781250\n",
            "Epoch 1: batch 10 of 112. Train loss: 1.503210 Accuracy: 0.437500\n",
            "Epoch 1: batch 10 of 112. \u001b[94mValidation\u001b[0m loss: 1.607629 Accuracy: 0.562500\n",
            "Epoch 1: batch 11 of 112. Train loss: 1.541305 Accuracy: 0.562500\n",
            "Epoch 1: batch 12 of 112. Train loss: 1.164567 Accuracy: 0.718750\n",
            "Epoch 1: batch 13 of 112. Train loss: 1.103079 Accuracy: 0.781250\n",
            "Epoch 1: batch 14 of 112. Train loss: 1.396749 Accuracy: 0.593750\n",
            "Epoch 1: batch 15 of 112. Train loss: 1.241505 Accuracy: 0.687500\n",
            "Epoch 1: batch 16 of 112. Train loss: 1.458795 Accuracy: 0.656250\n",
            "Epoch 1: batch 17 of 112. Train loss: 1.983818 Accuracy: 0.593750\n",
            "Epoch 1: batch 18 of 112. Train loss: 1.368950 Accuracy: 0.625000\n",
            "Epoch 1: batch 19 of 112. Train loss: 1.591542 Accuracy: 0.531250\n",
            "Epoch 1: batch 20 of 112. Train loss: 1.156965 Accuracy: 0.593750\n",
            "Epoch 1: batch 20 of 112. \u001b[94mValidation\u001b[0m loss: 1.250151 Accuracy: 0.562500\n",
            "Epoch 1: batch 21 of 112. Train loss: 1.640640 Accuracy: 0.531250\n",
            "Epoch 1: batch 22 of 112. Train loss: 1.402889 Accuracy: 0.625000\n",
            "Epoch 1: batch 23 of 112. Train loss: 1.880636 Accuracy: 0.468750\n",
            "Epoch 1: batch 24 of 112. Train loss: 1.183468 Accuracy: 0.781250\n",
            "Epoch 1: batch 25 of 112. Train loss: 1.510801 Accuracy: 0.562500\n",
            "Epoch 1: batch 26 of 112. Train loss: 1.254271 Accuracy: 0.656250\n",
            "Epoch 1: batch 27 of 112. Train loss: 1.105658 Accuracy: 0.625000\n",
            "Epoch 1: batch 28 of 112. Train loss: 1.288025 Accuracy: 0.656250\n",
            "Epoch 1: batch 29 of 112. Train loss: 1.000416 Accuracy: 0.750000\n",
            "Epoch 1: batch 30 of 112. Train loss: 1.939474 Accuracy: 0.500000\n",
            "Epoch 1: batch 30 of 112. \u001b[94mValidation\u001b[0m loss: 1.291375 Accuracy: 0.531250\n",
            "Epoch 1: batch 31 of 112. Train loss: 1.087798 Accuracy: 0.718750\n",
            "Epoch 1: batch 32 of 112. Train loss: 1.402190 Accuracy: 0.593750\n",
            "Epoch 1: batch 33 of 112. Train loss: 1.660029 Accuracy: 0.531250\n",
            "Epoch 1: batch 34 of 112. Train loss: 1.055382 Accuracy: 0.687500\n",
            "Epoch 1: batch 35 of 112. Train loss: 1.461659 Accuracy: 0.531250\n",
            "Epoch 1: batch 36 of 112. Train loss: 1.443240 Accuracy: 0.656250\n",
            "Epoch 1: batch 37 of 112. Train loss: 1.155236 Accuracy: 0.656250\n",
            "Epoch 1: batch 38 of 112. Train loss: 1.576621 Accuracy: 0.468750\n",
            "Epoch 1: batch 39 of 112. Train loss: 1.515725 Accuracy: 0.625000\n",
            "Epoch 1: batch 40 of 112. Train loss: 1.667704 Accuracy: 0.500000\n",
            "Epoch 1: batch 40 of 112. \u001b[94mValidation\u001b[0m loss: 1.383509 Accuracy: 0.406250\n",
            "Epoch 1: batch 41 of 112. Train loss: 1.340027 Accuracy: 0.562500\n",
            "Epoch 1: batch 42 of 112. Train loss: 1.368253 Accuracy: 0.687500\n",
            "Epoch 1: batch 43 of 112. Train loss: 1.308153 Accuracy: 0.531250\n",
            "Epoch 1: batch 44 of 112. Train loss: 1.353353 Accuracy: 0.656250\n",
            "Epoch 1: batch 45 of 112. Train loss: 1.083801 Accuracy: 0.750000\n",
            "Epoch 1: batch 46 of 112. Train loss: 1.148650 Accuracy: 0.656250\n",
            "Epoch 1: batch 47 of 112. Train loss: 1.079746 Accuracy: 0.687500\n",
            "Epoch 1: batch 48 of 112. Train loss: 1.117734 Accuracy: 0.656250\n",
            "Epoch 1: batch 49 of 112. Train loss: 1.020963 Accuracy: 0.687500\n",
            "Epoch 1: batch 50 of 112. Train loss: 1.583683 Accuracy: 0.625000\n",
            "Epoch 1: batch 50 of 112. \u001b[94mValidation\u001b[0m loss: 0.796451 Accuracy: 0.687500\n",
            "Epoch 1: batch 51 of 112. Train loss: 1.144917 Accuracy: 0.656250\n",
            "Epoch 1: batch 52 of 112. Train loss: 1.483645 Accuracy: 0.593750\n",
            "Epoch 1: batch 53 of 112. Train loss: 1.079802 Accuracy: 0.750000\n",
            "Epoch 1: batch 54 of 112. Train loss: 1.186949 Accuracy: 0.562500\n",
            "Epoch 1: batch 55 of 112. Train loss: 1.191482 Accuracy: 0.531250\n",
            "Epoch 1: batch 56 of 112. Train loss: 1.063530 Accuracy: 0.812500\n",
            "Epoch 1: batch 57 of 112. Train loss: 1.363676 Accuracy: 0.593750\n",
            "Epoch 1: batch 58 of 112. Train loss: 1.012969 Accuracy: 0.718750\n",
            "Epoch 1: batch 59 of 112. Train loss: 1.561829 Accuracy: 0.656250\n",
            "Epoch 1: batch 60 of 112. Train loss: 1.580436 Accuracy: 0.593750\n",
            "Epoch 1: batch 60 of 112. \u001b[94mValidation\u001b[0m loss: 1.463725 Accuracy: 0.406250\n",
            "Epoch 1: batch 61 of 112. Train loss: 1.526361 Accuracy: 0.656250\n",
            "Epoch 1: batch 62 of 112. Train loss: 1.341224 Accuracy: 0.593750\n",
            "Epoch 1: batch 63 of 112. Train loss: 1.533532 Accuracy: 0.562500\n",
            "Epoch 1: batch 64 of 112. Train loss: 1.375659 Accuracy: 0.750000\n",
            "Epoch 1: batch 65 of 112. Train loss: 1.116683 Accuracy: 0.781250\n",
            "Epoch 1: batch 66 of 112. Train loss: 1.353765 Accuracy: 0.593750\n",
            "Epoch 1: batch 67 of 112. Train loss: 1.421276 Accuracy: 0.593750\n",
            "Epoch 1: batch 68 of 112. Train loss: 2.091474 Accuracy: 0.531250\n",
            "Epoch 1: batch 69 of 112. Train loss: 1.311735 Accuracy: 0.781250\n",
            "Epoch 1: batch 70 of 112. Train loss: 1.587324 Accuracy: 0.656250\n",
            "Epoch 1: batch 70 of 112. \u001b[94mValidation\u001b[0m loss: 1.912527 Accuracy: 0.468750\n",
            "Epoch 1: batch 71 of 112. Train loss: 1.654345 Accuracy: 0.531250\n",
            "Epoch 1: batch 72 of 112. Train loss: 1.465368 Accuracy: 0.625000\n",
            "Epoch 1: batch 73 of 112. Train loss: 1.343882 Accuracy: 0.625000\n",
            "Epoch 1: batch 74 of 112. Train loss: 1.291588 Accuracy: 0.593750\n",
            "Epoch 1: batch 75 of 112. Train loss: 1.101232 Accuracy: 0.750000\n",
            "Epoch 1: batch 76 of 112. Train loss: 1.175903 Accuracy: 0.656250\n",
            "Epoch 1: batch 77 of 112. Train loss: 1.705302 Accuracy: 0.687500\n",
            "Epoch 1: batch 78 of 112. Train loss: 1.311146 Accuracy: 0.531250\n",
            "Epoch 1: batch 79 of 112. Train loss: 1.420630 Accuracy: 0.531250\n",
            "Epoch 1: batch 80 of 112. Train loss: 1.446319 Accuracy: 0.593750\n",
            "Epoch 1: batch 80 of 112. \u001b[94mValidation\u001b[0m loss: 1.326858 Accuracy: 0.468750\n",
            "Epoch 1: batch 81 of 112. Train loss: 1.726711 Accuracy: 0.437500\n",
            "Epoch 1: batch 82 of 112. Train loss: 1.361804 Accuracy: 0.562500\n",
            "Epoch 1: batch 83 of 112. Train loss: 1.736104 Accuracy: 0.562500\n",
            "Epoch 1: batch 84 of 112. Train loss: 1.881271 Accuracy: 0.468750\n",
            "Epoch 1: batch 85 of 112. Train loss: 1.912951 Accuracy: 0.562500\n",
            "Epoch 1: batch 86 of 112. Train loss: 1.173602 Accuracy: 0.687500\n",
            "Epoch 1: batch 87 of 112. Train loss: 1.358350 Accuracy: 0.500000\n",
            "Epoch 1: batch 88 of 112. Train loss: 1.321579 Accuracy: 0.687500\n",
            "Epoch 1: batch 89 of 112. Train loss: 1.351832 Accuracy: 0.625000\n",
            "Epoch 1: batch 90 of 112. Train loss: 1.139592 Accuracy: 0.687500\n",
            "Epoch 1: batch 90 of 112. \u001b[94mValidation\u001b[0m loss: 1.895776 Accuracy: 0.468750\n",
            "Epoch 1: batch 91 of 112. Train loss: 1.227652 Accuracy: 0.593750\n",
            "Epoch 1: batch 92 of 112. Train loss: 1.243105 Accuracy: 0.687500\n",
            "Epoch 1: batch 93 of 112. Train loss: 1.468305 Accuracy: 0.750000\n",
            "Epoch 1: batch 94 of 112. Train loss: 1.520897 Accuracy: 0.531250\n",
            "Epoch 1: batch 95 of 112. Train loss: 1.923717 Accuracy: 0.593750\n",
            "Epoch 1: batch 96 of 112. Train loss: 2.237263 Accuracy: 0.531250\n",
            "Epoch 1: batch 97 of 112. Train loss: 1.309187 Accuracy: 0.687500\n",
            "Epoch 1: batch 98 of 112. Train loss: 1.363923 Accuracy: 0.718750\n",
            "Epoch 1: batch 99 of 112. Train loss: 1.600948 Accuracy: 0.687500\n",
            "Epoch 1: batch 100 of 112. Train loss: 1.009059 Accuracy: 0.843750\n",
            "Epoch 1: batch 100 of 112. \u001b[94mValidation\u001b[0m loss: 1.273584 Accuracy: 0.593750\n",
            "Epoch 1: batch 101 of 112. Train loss: 1.270065 Accuracy: 0.625000\n",
            "Epoch 1: batch 102 of 112. Train loss: 1.403798 Accuracy: 0.718750\n",
            "Epoch 1: batch 103 of 112. Train loss: 1.802406 Accuracy: 0.531250\n",
            "Epoch 1: batch 104 of 112. Train loss: 1.010666 Accuracy: 0.812500\n",
            "Epoch 1: batch 105 of 112. Train loss: 1.197110 Accuracy: 0.687500\n",
            "Epoch 1: batch 106 of 112. Train loss: 1.110578 Accuracy: 0.625000\n",
            "Epoch 1: batch 107 of 112. Train loss: 1.354347 Accuracy: 0.625000\n",
            "Epoch 1: batch 108 of 112. Train loss: 1.209400 Accuracy: 0.687500\n",
            "Epoch 1: batch 109 of 112. Train loss: 1.489500 Accuracy: 0.625000\n",
            "Epoch 1: batch 110 of 112. Train loss: 1.244953 Accuracy: 0.812500\n",
            "Epoch 1: batch 110 of 112. \u001b[94mValidation\u001b[0m loss: 1.575489 Accuracy: 0.500000\n",
            "Epoch 1: batch 111 of 112. Train loss: 1.789838 Accuracy: 0.625000\n",
            "Epoch 1: batch 112 of 112. Train loss: 1.715130 Accuracy: 0.125000\n",
            "Epoch 2: batch 0 of 112. Train loss: 1.542669 Accuracy: 0.687500\n",
            "Epoch 2: batch 0 of 112. \u001b[94mValidation\u001b[0m loss: 0.988056 Accuracy: 0.562500\n",
            "Epoch 2: batch 1 of 112. Train loss: 1.419435 Accuracy: 0.625000\n",
            "Epoch 2: batch 2 of 112. Train loss: 1.241446 Accuracy: 0.718750\n",
            "Epoch 2: batch 3 of 112. Train loss: 1.050047 Accuracy: 0.750000\n",
            "Epoch 2: batch 4 of 112. Train loss: 1.311120 Accuracy: 0.718750\n",
            "Epoch 2: batch 5 of 112. Train loss: 1.172241 Accuracy: 0.531250\n",
            "Epoch 2: batch 6 of 112. Train loss: 1.070809 Accuracy: 0.718750\n",
            "Epoch 2: batch 7 of 112. Train loss: 1.368426 Accuracy: 0.812500\n",
            "Epoch 2: batch 8 of 112. Train loss: 1.036431 Accuracy: 0.718750\n",
            "Epoch 2: batch 9 of 112. Train loss: 1.284632 Accuracy: 0.656250\n",
            "Epoch 2: batch 10 of 112. Train loss: 1.052238 Accuracy: 0.718750\n",
            "Epoch 2: batch 10 of 112. \u001b[94mValidation\u001b[0m loss: 1.099409 Accuracy: 0.593750\n",
            "Epoch 2: batch 11 of 112. Train loss: 1.170610 Accuracy: 0.656250\n",
            "Epoch 2: batch 12 of 112. Train loss: 1.743151 Accuracy: 0.531250\n",
            "Epoch 2: batch 13 of 112. Train loss: 1.558061 Accuracy: 0.500000\n",
            "Epoch 2: batch 14 of 112. Train loss: 1.427031 Accuracy: 0.593750\n",
            "Epoch 2: batch 15 of 112. Train loss: 0.976361 Accuracy: 0.687500\n",
            "Epoch 2: batch 16 of 112. Train loss: 1.586892 Accuracy: 0.468750\n",
            "Epoch 2: batch 17 of 112. Train loss: 1.459312 Accuracy: 0.593750\n",
            "Epoch 2: batch 18 of 112. Train loss: 1.737525 Accuracy: 0.531250\n",
            "Epoch 2: batch 19 of 112. Train loss: 1.934919 Accuracy: 0.687500\n",
            "Epoch 2: batch 20 of 112. Train loss: 2.060286 Accuracy: 0.718750\n",
            "Epoch 2: batch 20 of 112. \u001b[94mValidation\u001b[0m loss: 2.723812 Accuracy: 0.406250\n",
            "Epoch 2: batch 21 of 112. Train loss: 2.291018 Accuracy: 0.500000\n",
            "Epoch 2: batch 22 of 112. Train loss: 2.770236 Accuracy: 0.500000\n",
            "Epoch 2: batch 23 of 112. Train loss: 2.321093 Accuracy: 0.718750\n",
            "Epoch 2: batch 24 of 112. Train loss: 2.198637 Accuracy: 0.593750\n",
            "Epoch 2: batch 25 of 112. Train loss: 2.171472 Accuracy: 0.531250\n",
            "Epoch 2: batch 26 of 112. Train loss: 1.972675 Accuracy: 0.531250\n",
            "Epoch 2: batch 27 of 112. Train loss: 1.626953 Accuracy: 0.687500\n",
            "Epoch 2: batch 28 of 112. Train loss: 1.581144 Accuracy: 0.718750\n",
            "Epoch 2: batch 29 of 112. Train loss: 1.626427 Accuracy: 0.625000\n",
            "Epoch 2: batch 30 of 112. Train loss: 1.995584 Accuracy: 0.500000\n",
            "Epoch 2: batch 30 of 112. \u001b[94mValidation\u001b[0m loss: 1.259857 Accuracy: 0.593750\n",
            "Epoch 2: batch 31 of 112. Train loss: 1.262017 Accuracy: 0.718750\n",
            "Epoch 2: batch 32 of 112. Train loss: 1.695058 Accuracy: 0.593750\n",
            "Epoch 2: batch 33 of 112. Train loss: 1.299509 Accuracy: 0.593750\n",
            "Epoch 2: batch 34 of 112. Train loss: 1.452170 Accuracy: 0.687500\n",
            "Epoch 2: batch 35 of 112. Train loss: 1.447382 Accuracy: 0.625000\n",
            "Epoch 2: batch 36 of 112. Train loss: 1.297586 Accuracy: 0.781250\n",
            "Epoch 2: batch 37 of 112. Train loss: 1.711621 Accuracy: 0.562500\n",
            "Epoch 2: batch 38 of 112. Train loss: 1.453547 Accuracy: 0.593750\n",
            "Epoch 2: batch 39 of 112. Train loss: 1.507611 Accuracy: 0.625000\n",
            "Epoch 2: batch 40 of 112. Train loss: 1.541021 Accuracy: 0.593750\n",
            "Epoch 2: batch 40 of 112. \u001b[94mValidation\u001b[0m loss: 2.232000 Accuracy: 0.406250\n",
            "Epoch 2: batch 41 of 112. Train loss: 1.976293 Accuracy: 0.593750\n",
            "Epoch 2: batch 42 of 112. Train loss: 1.377701 Accuracy: 0.687500\n",
            "Epoch 2: batch 43 of 112. Train loss: 1.185609 Accuracy: 0.687500\n",
            "Epoch 2: batch 44 of 112. Train loss: 1.312042 Accuracy: 0.562500\n",
            "Epoch 2: batch 45 of 112. Train loss: 1.205252 Accuracy: 0.718750\n",
            "Epoch 2: batch 46 of 112. Train loss: 1.342404 Accuracy: 0.625000\n",
            "Epoch 2: batch 47 of 112. Train loss: 1.423453 Accuracy: 0.656250\n",
            "Epoch 2: batch 48 of 112. Train loss: 1.199452 Accuracy: 0.593750\n",
            "Epoch 2: batch 49 of 112. Train loss: 1.355850 Accuracy: 0.562500\n",
            "Epoch 2: batch 50 of 112. Train loss: 1.080236 Accuracy: 0.687500\n",
            "Epoch 2: batch 50 of 112. \u001b[94mValidation\u001b[0m loss: 1.400082 Accuracy: 0.562500\n",
            "Epoch 2: batch 51 of 112. Train loss: 0.983941 Accuracy: 0.812500\n",
            "Epoch 2: batch 52 of 112. Train loss: 1.117310 Accuracy: 0.625000\n",
            "Epoch 2: batch 53 of 112. Train loss: 0.859206 Accuracy: 0.781250\n",
            "Epoch 2: batch 54 of 112. Train loss: 1.155084 Accuracy: 0.781250\n",
            "Epoch 2: batch 55 of 112. Train loss: 1.550156 Accuracy: 0.562500\n",
            "Epoch 2: batch 56 of 112. Train loss: 1.054093 Accuracy: 0.625000\n",
            "Epoch 2: batch 57 of 112. Train loss: 1.012688 Accuracy: 0.750000\n",
            "Epoch 2: batch 58 of 112. Train loss: 1.173622 Accuracy: 0.687500\n",
            "Epoch 2: batch 59 of 112. Train loss: 1.125058 Accuracy: 0.625000\n",
            "Epoch 2: batch 60 of 112. Train loss: 1.696036 Accuracy: 0.593750\n",
            "Epoch 2: batch 60 of 112. \u001b[94mValidation\u001b[0m loss: 1.141975 Accuracy: 0.531250\n",
            "Epoch 2: batch 61 of 112. Train loss: 1.068836 Accuracy: 0.718750\n",
            "Epoch 2: batch 62 of 112. Train loss: 1.258780 Accuracy: 0.656250\n",
            "Epoch 2: batch 63 of 112. Train loss: 1.516918 Accuracy: 0.562500\n",
            "Epoch 2: batch 64 of 112. Train loss: 1.365365 Accuracy: 0.593750\n",
            "Epoch 2: batch 65 of 112. Train loss: 1.174546 Accuracy: 0.656250\n",
            "Epoch 2: batch 66 of 112. Train loss: 1.254143 Accuracy: 0.687500\n",
            "Epoch 2: batch 67 of 112. Train loss: 1.404625 Accuracy: 0.687500\n",
            "Epoch 2: batch 68 of 112. Train loss: 1.083274 Accuracy: 0.812500\n",
            "Epoch 2: batch 69 of 112. Train loss: 1.610917 Accuracy: 0.718750\n",
            "Epoch 2: batch 70 of 112. Train loss: 1.318134 Accuracy: 0.687500\n",
            "Epoch 2: batch 70 of 112. \u001b[94mValidation\u001b[0m loss: 1.939985 Accuracy: 0.500000\n",
            "Epoch 2: batch 71 of 112. Train loss: 1.307562 Accuracy: 0.687500\n",
            "Epoch 2: batch 72 of 112. Train loss: 0.959976 Accuracy: 0.812500\n",
            "Epoch 2: batch 73 of 112. Train loss: 1.092257 Accuracy: 0.718750\n",
            "Epoch 2: batch 74 of 112. Train loss: 1.134448 Accuracy: 0.687500\n",
            "Epoch 2: batch 75 of 112. Train loss: 1.195236 Accuracy: 0.625000\n",
            "Epoch 2: batch 76 of 112. Train loss: 0.942354 Accuracy: 0.718750\n",
            "Epoch 2: batch 77 of 112. Train loss: 1.245486 Accuracy: 0.625000\n",
            "Epoch 2: batch 78 of 112. Train loss: 0.950403 Accuracy: 0.781250\n",
            "Epoch 2: batch 79 of 112. Train loss: 1.457836 Accuracy: 0.593750\n",
            "Epoch 2: batch 80 of 112. Train loss: 1.207218 Accuracy: 0.656250\n",
            "Epoch 2: batch 80 of 112. \u001b[94mValidation\u001b[0m loss: 0.761343 Accuracy: 0.687500\n",
            "Epoch 2: batch 81 of 112. Train loss: 1.216875 Accuracy: 0.687500\n",
            "Epoch 2: batch 82 of 112. Train loss: 1.256377 Accuracy: 0.687500\n",
            "Epoch 2: batch 83 of 112. Train loss: 1.902727 Accuracy: 0.468750\n",
            "Epoch 2: batch 84 of 112. Train loss: 1.908407 Accuracy: 0.500000\n",
            "Epoch 2: batch 85 of 112. Train loss: 1.369314 Accuracy: 0.625000\n",
            "Epoch 2: batch 86 of 112. Train loss: 1.476028 Accuracy: 0.718750\n",
            "Epoch 2: batch 87 of 112. Train loss: 1.436684 Accuracy: 0.593750\n",
            "Epoch 2: batch 88 of 112. Train loss: 1.251084 Accuracy: 0.718750\n",
            "Epoch 2: batch 89 of 112. Train loss: 1.450973 Accuracy: 0.593750\n",
            "Epoch 2: batch 90 of 112. Train loss: 1.286174 Accuracy: 0.437500\n",
            "Epoch 2: batch 90 of 112. \u001b[94mValidation\u001b[0m loss: 0.935493 Accuracy: 0.718750\n",
            "Epoch 2: batch 91 of 112. Train loss: 0.942302 Accuracy: 0.687500\n",
            "Epoch 2: batch 92 of 112. Train loss: 1.061587 Accuracy: 0.718750\n",
            "Epoch 2: batch 93 of 112. Train loss: 1.381473 Accuracy: 0.625000\n",
            "Epoch 2: batch 94 of 112. Train loss: 1.303148 Accuracy: 0.687500\n",
            "Epoch 2: batch 95 of 112. Train loss: 1.103126 Accuracy: 0.656250\n",
            "Epoch 2: batch 96 of 112. Train loss: 1.341313 Accuracy: 0.625000\n",
            "Epoch 2: batch 97 of 112. Train loss: 1.388760 Accuracy: 0.656250\n",
            "Epoch 2: batch 98 of 112. Train loss: 1.081702 Accuracy: 0.625000\n",
            "Epoch 2: batch 99 of 112. Train loss: 1.448590 Accuracy: 0.562500\n",
            "Epoch 2: batch 100 of 112. Train loss: 1.879136 Accuracy: 0.531250\n",
            "Epoch 2: batch 100 of 112. \u001b[94mValidation\u001b[0m loss: 1.665030 Accuracy: 0.593750\n",
            "Epoch 2: batch 101 of 112. Train loss: 1.130970 Accuracy: 0.625000\n",
            "Epoch 2: batch 102 of 112. Train loss: 0.983821 Accuracy: 0.781250\n",
            "Epoch 2: batch 103 of 112. Train loss: 1.445878 Accuracy: 0.593750\n",
            "Epoch 2: batch 104 of 112. Train loss: 1.323766 Accuracy: 0.718750\n",
            "Epoch 2: batch 105 of 112. Train loss: 1.443838 Accuracy: 0.656250\n",
            "Epoch 2: batch 106 of 112. Train loss: 1.347057 Accuracy: 0.562500\n",
            "Epoch 2: batch 107 of 112. Train loss: 0.963391 Accuracy: 0.812500\n",
            "Epoch 2: batch 108 of 112. Train loss: 0.838306 Accuracy: 0.812500\n",
            "Epoch 2: batch 109 of 112. Train loss: 0.966013 Accuracy: 0.718750\n",
            "Epoch 2: batch 110 of 112. Train loss: 1.154953 Accuracy: 0.593750\n",
            "Epoch 2: batch 110 of 112. \u001b[94mValidation\u001b[0m loss: 1.125691 Accuracy: 0.562500\n",
            "Epoch 2: batch 111 of 112. Train loss: 1.159320 Accuracy: 0.656250\n",
            "Epoch 2: batch 112 of 112. Train loss: 2.902431 Accuracy: 0.093750\n",
            "\u001b[94mTesting\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-dc7e2519f3c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m###################points, target = points.cuda(), target.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mpred_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_choice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-8aeccebe1236>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-ceeec47c4a76>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mn_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#ipdb.set_trace(context=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-f81f4b7f4e75>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2446\u001b[0m         )\n\u001b[1;32m   2447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2448\u001b[0;31m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2450\u001b[0m     return torch.batch_norm(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2414\u001b[0m         \u001b[0msize_prods\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2415\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2416\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected more than 1 value per channel when training, got input size {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 512])"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m(2416)\u001b[0;36m_verify_batch_size\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   2414 \u001b[0;31m        \u001b[0msize_prods\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   2415 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m-> 2416 \u001b[0;31m        \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected more than 1 value per channel when training, got input size {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   2417 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   2418 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> Q\n",
            "*** NameError: name 'Q' is not defined\n",
            "ipdb> q\n"
          ]
        }
      ],
      "source": [
        "conf = OmegaConf.load('classification.yaml')\n",
        "options = instantiate(conf)\n",
        "\n",
        "blue = lambda x: '\\033[94m' + x + '\\033[0m'\n",
        "\n",
        "options.manualSeed = random.randint(1, 10000)  # fix seed\n",
        "print(\"Random Seed: \", options.manualSeed)\n",
        "random.seed(options.manualSeed)\n",
        "torch.manual_seed(options.manualSeed)\n",
        "\n",
        "dataset = ModelNetLoader(root=options.dataset, num_classes=options.num_classes, batch_size=options.batch_size, split='train', numpoints=options.num_points)\n",
        "testdataset = ModelNetLoader(root=options.dataset, num_classes=options.num_classes, batch_size=options.batch_size, split='test', numpoints=options.num_points)\n",
        "\n",
        "traindataset_size = int(len(dataset) * 0.9)\n",
        "traindataset, valdataset = random_split(dataset, [traindataset_size, len(dataset) - traindataset_size])\n",
        "       \n",
        "print(\"Rozmiar zbioru treningowego: \" + str(len(traindataset)))\n",
        "print(\"Rozmiar zbioru walidacyjnego: \" + str(len(valdataset)))\n",
        "print(\"Rozmiar zbioru testowego: \" + str(len(testdataset)))\n",
        "print(\"Rozmiar batcha: \" + str(dataset.batch_size))\n",
        "print('Liczba klas: ' + str(dataset.num_classes))\n",
        "\n",
        "traindataloader = DataLoader(traindataset, batch_size=options.batch_size, num_workers=int(options.num_workers), shuffle=True)\n",
        "valdataloader = DataLoader(valdataset, batch_size=options.batch_size, num_workers=int(options.num_workers), shuffle=True)\n",
        "testdataloader = DataLoader(testdataset, batch_size=1, num_workers=int(options.num_workers), shuffle=False)\n",
        "\n",
        "try:\n",
        "    os.makedirs(options.out_folder)\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "classifier = PointNetCls(k=dataset.num_classes, feature_transform=options.feature_transform)\n",
        "\n",
        "if options.model != '':\n",
        "    classifier.load_state_dict(torch.load(options.model))\n",
        "\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "################classifier.cuda()\n",
        "\n",
        "num_batch = len(traindataset) / options.batch_size\n",
        "\n",
        "'''\n",
        "single_batch_overfit = torch.empty((options.batch_size, options.num_points, 3))\n",
        "single_batch_overfit_cls = torch.empty((options.batch_size))\n",
        "\n",
        "for n in range(options.batch_size):\n",
        "  item = dataset[100*n]\n",
        "  single_batch_overfit[n, :, :] = item[0]\n",
        "  single_batch_overfit_cls[n] = item[1]\n",
        "single_batch_overfit_cls = single_batch_overfit_cls.type(torch.int64)\n",
        "'''\n",
        "\n",
        "for epoch in range(options.num_epochs):\n",
        "    scheduler.step()\n",
        "    for i, data_ in enumerate(traindataloader, 0): #fir element in enumerate\n",
        "        points, target = data_\n",
        "        #points, target = single_batch_overfit, single_batch_overfit_cls\n",
        "        #ipdb.set_trace(context=5)\n",
        "        #target = target[:, 0]\n",
        "        points = points.transpose(2, 1)\n",
        "        ####################points, target = points.cuda(), target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        classifier = classifier.train()\n",
        "        pred, trans, trans_feat = classifier(points)\n",
        "        loss = F.nll_loss(pred, target)\n",
        "        if options.feature_transform:\n",
        "            loss += feature_transform_regularizer(trans_feat) * 0.001\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pred_choice = pred.data.max(1)[1]\n",
        "        correct = pred_choice.eq(target.data).cpu().sum()\n",
        "        print('Epoch %d: batch %d of %d. Train loss: %f Accuracy: %f' % (epoch, i, num_batch, loss.item(), correct.item() / float(options.batch_size)))\n",
        "        \n",
        "        if i % 10 == 0:\n",
        "            j, data_ = next(enumerate(valdataloader, 0))\n",
        "            points, target = data_\n",
        "            #points, target = single_batch_overfit, single_batch_overfit_cls\n",
        "            #target = target[:, 0]\n",
        "            points = points.transpose(2, 1)\n",
        "            ###########################points, target = points.cuda(), target.cuda()\n",
        "            classifier = classifier.eval()\n",
        "            pred, _, _ = classifier(points)\n",
        "            loss = F.nll_loss(pred, target)\n",
        "            pred_choice = pred.data.max(1)[1]\n",
        "            correct = pred_choice.eq(target.data).cpu().sum()\n",
        "            print('Epoch %d: batch %d of %d. %s loss: %f Accuracy: %f' % (epoch, i, num_batch, blue('Validation'), loss.item(), correct.item()/float(options.batch_size)))\n",
        "        \n",
        "    torch.save(classifier.state_dict(), '%s/cls_model_%d.pth' % (options.out_folder, epoch))\n",
        "\n",
        "total_correct = 0\n",
        "total_testset = 0\n",
        "print(blue(\"Testing\"))\n",
        "\n",
        "for i,data_ in tqdm(enumerate(testdataloader, 0)):\n",
        "    points, target = data_\n",
        "    #target = target[:, 0]\n",
        "    points = points.transpose(2, 1)\n",
        "    ###################points, target = points.cuda(), target.cuda()\n",
        "    classifier = classifier.eval()\n",
        "    pred, _, _ = classifier(points)\n",
        "    pred_choice = pred.data.max(1)[1]\n",
        "    correct = pred_choice.eq(target.data).cpu().sum()\n",
        "    total_correct += correct.item()\n",
        "    total_testset += points.size()[0]\n",
        "\n",
        "print(\"\\nFinal accuracy: {}\".format(total_correct / float(total_testset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_SDetaWIxsQ"
      },
      "source": [
        "```python\n",
        "class STN3d(nn.Module):\n",
        "  output:\n",
        "    stn torch.Size([32, 3, 3])global feat torch.Size([32, 1024])classification head:\n",
        "      global feat torch.Size([32, 1024])\n",
        "      point feat torch.Size([32, 1088, 2500])\n",
        "      class torch.Size([32, 5])segmentation head:\n",
        "        PointNetDenseCls(\n",
        "          (feat): PointNetfeat(\n",
        "            (stn): STN3d(\n",
        "              (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
        "              (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
        "              (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
        "              (mp1): MaxPool1d(kernel_size=2500, stride=2500, padding=0, dilation=1, ceil_mode=False)\n",
        "              (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
        "              (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
        "              (fc3): Linear(in_features=256, out_features=9, bias=True)\n",
        "              (relu): ReLU()\n",
        "              (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "              (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "              (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "              (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "              (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "            )\n",
        "            (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
        "            (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
        "            (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
        "            (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "            (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "            (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "            (mp1): MaxPool1d(kernel_size=2500, stride=2500, padding=0, dilation=1, ceil_mode=False)\n",
        "          )\n",
        "          (conv1): Conv1d(1088, 512, kernel_size=(1,), stride=(1,))\n",
        "          (conv2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
        "          (conv3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
        "          (conv4): Conv1d(128, 3, kernel_size=(1,), stride=(1,))\n",
        "          (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "          (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "          (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        )\n",
        "  seg torch.Size([32, 2500, 3])\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7yvJCbVGZmgL",
        "M4E0KWaQZt_8"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

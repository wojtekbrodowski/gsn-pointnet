{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WjtBCQQ9gzIR",
    "outputId": "57e29129-f301-476b-97c5-0999488870cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 761 kB 39.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 61.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 386 kB 75.5 MB/s \n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 8.7.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install ipdb -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3T1lRg05bJCu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import ipdb\n",
    "\n",
    "class STN3d(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(STN3d, self).__init__()\n",
    "        self.debug1='debug1'\n",
    "        print('debug1')\n",
    "        #ipdb.set_trace(context=5)\n",
    "        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n",
    "        print('debug2')\n",
    "        #ipdb.set_trace(context=5)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 9)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64) #To jest ta normalizacja odchylenia i sredniej (?1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #ipdb.set_trace(context=5)\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024) #to jest jakiś resize...(?2)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        #ipdb.set_trace(context=5)\n",
    "        iden = Variable(torch.from_numpy(np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32))).view(1, 9).repeat(\n",
    "            batchsize, 1)\n",
    "        if x.is_cuda:\n",
    "            iden = iden.cuda()\n",
    "        x = x + iden\n",
    "        x = x.view(-1, 3, 3)\n",
    "        return x\n",
    "\n",
    "class STNkd(nn.Module):\n",
    "    def __init__(self, k=64):\n",
    "        super(STNkd, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k*k)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1,self.k*self.k).repeat(batchsize,1)\n",
    "        if x.is_cuda:\n",
    "            iden = iden.cuda()\n",
    "        x = x + iden\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x\n",
    "\n",
    "class PointNetfeat(nn.Module):\n",
    "    def __init__(self, global_feat = True, feature_transform = False):\n",
    "        super(PointNetfeat, self).__init__()\n",
    "        self.stn = STN3d(channel=3)\n",
    "        #self.stn = STN3d()\n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.global_feat = global_feat\n",
    "        self.feature_transform = feature_transform\n",
    "        if self.feature_transform:\n",
    "            self.fstn = STNkd(k=64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_pts = x.size()[2]\n",
    "        trans = self.stn(x)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = torch.bmm(x, trans)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        if self.feature_transform:\n",
    "            trans_feat = self.fstn(x)\n",
    "            x = x.transpose(2,1)\n",
    "            x = torch.bmm(x, trans_feat)\n",
    "            x = x.transpose(2,1)\n",
    "        else:\n",
    "            trans_feat = None\n",
    "\n",
    "        pointfeat = x\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "        if self.global_feat:\n",
    "            return x, trans, trans_feat\n",
    "        else:\n",
    "            x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
    "            return torch.cat([x, pointfeat], 1), trans, trans_feat\n",
    "\n",
    "class PointNetCls(nn.Module):\n",
    "    def __init__(self, k=2, feature_transform=False):\n",
    "        super(PointNetCls, self).__init__()\n",
    "        self.feature_transform = feature_transform\n",
    "        self.feat = PointNetfeat(global_feat=True, feature_transform=feature_transform)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, trans, trans_feat = self.feat(x)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.dropout(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1), trans, trans_feat\n",
    "\n",
    "class PointNetDenseCls(nn.Module):\n",
    "    def __init__(self, k = 2, feature_transform=False):\n",
    "        super(PointNetDenseCls, self).__init__()\n",
    "        self.k = k\n",
    "        self.feature_transform=feature_transform\n",
    "        self.feat = PointNetfeat(global_feat=False, feature_transform=feature_transform)\n",
    "        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        n_pts = x.size()[2]\n",
    "        x, trans, trans_feat = self.feat(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.conv4(x)\n",
    "        x = x.transpose(2,1).contiguous()\n",
    "        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n",
    "        x = x.view(batchsize, n_pts, self.k)\n",
    "        return x, trans, trans_feat\n",
    "\n",
    "def feature_transform_regularizer(trans):\n",
    "    d = trans.size()[1]\n",
    "    batchsize = trans.size()[0]\n",
    "    I = torch.eye(d)[None, :, :]\n",
    "    if trans.is_cuda:\n",
    "        I = I.cuda()\n",
    "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEopUc4tn7rS",
    "outputId": "05c3e0fe-0f74-4c28-a3e4-1c1a3582118e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting plyfile\n",
      "  Downloading plyfile-0.7.4-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.8/dist-packages (from plyfile) (1.21.6)\n",
      "Installing collected packages: plyfile\n",
      "Successfully installed plyfile-0.7.4\n"
     ]
    }
   ],
   "source": [
    "!pip install plyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kTY2vmIvo-BU",
    "outputId": "b90526b1-29cb-464b-8cbf-94ea5d6f9425"
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "mkdir ./misc\n",
    "echo \"Airplane\t4\n",
    "Bag\t2\n",
    "Cap\t2\n",
    "Car\t4\n",
    "Chair\t4\n",
    "Earphone\t3\n",
    "Guitar\t3\n",
    "Knife\t2\n",
    "Lamp\t4\n",
    "Laptop\t2\n",
    "Motorbike\t6\n",
    "Mug\t2\n",
    "Pistol\t3\n",
    "Rocket\t3\n",
    "Skateboard\t3\n",
    "Table\t3\" > ./misc/num_seg_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmooTYS_psF2",
    "outputId": "1c914d1e-2fc5-4ae5-8a07-5441380ef532"
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "echo \"airplane\t0\n",
    "bathtub\t1\n",
    "bed\t2\n",
    "bench\t3\n",
    "bookshelf\t4\n",
    "bottle\t5\n",
    "bowl\t6\n",
    "car\t7\n",
    "chair\t8\n",
    "cone\t9\n",
    "cup\t10\n",
    "curtain\t11\n",
    "desk\t12\n",
    "door\t13\n",
    "dresser\t14\n",
    "flower_pot\t15\n",
    "glass_box\t16\n",
    "guitar\t17\n",
    "keyboard\t18\n",
    "lamp\t19\n",
    "laptop\t20\n",
    "mantel\t21\n",
    "monitor\t22\n",
    "night_stand\t23\n",
    "person\t24\n",
    "piano\t25\n",
    "plant\t26\n",
    "radio\t27\n",
    "range_hood\t28\n",
    "sink\t29\n",
    "sofa\t30\n",
    "stairs\t31\n",
    "stool\t32\n",
    "table\t33\n",
    "tent\t34\n",
    "toilet\t35\n",
    "tv_stand\t36\n",
    "vase\t37\n",
    "wardrobe\t38\n",
    "xbox\t39\" > ./misc/modelnet_id.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5ZzJxda71pV"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "wget https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip --no-check-certificate\n",
    "unzip modelnet40_ply_hdf5_2048.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNpd19II--vf"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "wget http://modelnet.cs.princeton.edu/ModelNet40.zip\n",
    "unzip ModelNet40.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kr-R74wmyaka"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "wget https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_segmentation_benchmark_v0.zip --no-check-certificate\n",
    "unzip shapenetcore_partanno_segmentation_benchmark_v0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "p9WzFLbwoTkD"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm \n",
    "import json\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "def get_segmentation_classes(root):\n",
    "    catfile = os.path.join(root, 'synsetoffset2category.txt')\n",
    "    cat = {}\n",
    "    meta = {}\n",
    "\n",
    "    with open(catfile, 'r') as f:\n",
    "        for line in f:\n",
    "            ls = line.strip().split()\n",
    "            cat[ls[0]] = ls[1]\n",
    "\n",
    "    for item in cat:\n",
    "        dir_seg = os.path.join(root, cat[item], 'points_label')\n",
    "        dir_point = os.path.join(root, cat[item], 'points')\n",
    "        fns = sorted(os.listdir(dir_point))\n",
    "        meta[item] = []\n",
    "        for fn in fns:\n",
    "            token = (os.path.splitext(os.path.basename(fn))[0])\n",
    "            meta[item].append((os.path.join(dir_point, token + '.pts'), os.path.join(dir_seg, token + '.seg')))\n",
    "    \n",
    "    with open('./misc/num_seg_classes.txt', 'w') as f:\n",
    "        for item in cat:\n",
    "            datapath = []\n",
    "            num_seg_classes = 0\n",
    "            for fn in meta[item]:\n",
    "                datapath.append((item, fn[0], fn[1]))\n",
    "\n",
    "            for i in tqdm(range(len(datapath))):\n",
    "                l = len(np.unique(np.loadtxt(datapath[i][-1]).astype(np.uint8)))\n",
    "                if l > num_seg_classes:\n",
    "                    num_seg_classes = l\n",
    "\n",
    "            print(\"category {} num segmentation classes {}\".format(item, num_seg_classes))\n",
    "            f.write(\"{}\\t{}\\n\".format(item, num_seg_classes))\n",
    "\n",
    "def gen_modelnet_id(root):\n",
    "    classes = []\n",
    "    with open(os.path.join(root, 'train.txt'), 'r') as f:\n",
    "        for line in f:\n",
    "            classes.append(line.strip().split('/')[0])\n",
    "    classes = np.unique(classes)\n",
    "    with open('./misc/modelnet_id.txt', 'w') as f:\n",
    "        for i in range(len(classes)):\n",
    "            f.write('{}\\t{}\\n'.format(classes[i], i))\n",
    "\n",
    "class ShapeNetDataset(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 npoints=2500,\n",
    "                 classification=False,\n",
    "                 class_choice=None,\n",
    "                 split='train',\n",
    "                 data_augmentation=True):\n",
    "        self.npoints = npoints\n",
    "        self.root = root\n",
    "        self.catfile = os.path.join(self.root, 'synsetoffset2category.txt')\n",
    "        self.cat = {}\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.classification = classification\n",
    "        self.seg_classes = {}\n",
    "        \n",
    "        with open(self.catfile, 'r') as f:\n",
    "            for line in f:\n",
    "                ls = line.strip().split()\n",
    "                self.cat[ls[0]] = ls[1]\n",
    "        #print(self.cat)\n",
    "        if not class_choice is None:\n",
    "            self.cat = {k: v for k, v in self.cat.items() if k in class_choice}\n",
    "\n",
    "        self.id2cat = {v: k for k, v in self.cat.items()}\n",
    "\n",
    "        self.meta = {}\n",
    "        splitfile = os.path.join(self.root, 'train_test_split', 'shuffled_{}_file_list.json'.format(split))\n",
    "        #from IPython import embed; embed()\n",
    "        filelist = json.load(open(splitfile, 'r'))\n",
    "        for item in self.cat:\n",
    "            self.meta[item] = []\n",
    "\n",
    "        for file in filelist:\n",
    "            _, category, uuid = file.split('/')\n",
    "            if category in self.cat.values():\n",
    "                self.meta[self.id2cat[category]].append((os.path.join(self.root, category, 'points', uuid+'.pts'),\n",
    "                                        os.path.join(self.root, category, 'points_label', uuid+'.seg')))\n",
    "\n",
    "        self.datapath = []\n",
    "        for item in self.cat:\n",
    "            for fn in self.meta[item]:\n",
    "                self.datapath.append((item, fn[0], fn[1]))\n",
    "\n",
    "        self.classes = dict(zip(sorted(self.cat), range(len(self.cat))))\n",
    "        print(self.classes)\n",
    "        with open('./misc/num_seg_classes.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                ls = line.strip().split()\n",
    "                self.seg_classes[ls[0]] = int(ls[1])\n",
    "        self.num_seg_classes = self.seg_classes[list(self.cat.keys())[0]]\n",
    "        print(self.seg_classes, self.num_seg_classes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn = self.datapath[index]\n",
    "        cls = self.classes[self.datapath[index][0]]\n",
    "        point_set = np.loadtxt(fn[1]).astype(np.float32)\n",
    "        seg = np.loadtxt(fn[2]).astype(np.int64)\n",
    "        #print(point_set.shape, seg.shape)\n",
    "\n",
    "        choice = np.random.choice(len(seg), self.npoints, replace=True)\n",
    "        #resample\n",
    "        point_set = point_set[choice, :]\n",
    "\n",
    "        point_set = point_set - np.expand_dims(np.mean(point_set, axis = 0), 0) # center\n",
    "        dist = np.max(np.sqrt(np.sum(point_set ** 2, axis = 1)),0)\n",
    "        point_set = point_set / dist #scale\n",
    "\n",
    "        if self.data_augmentation:\n",
    "            theta = np.random.uniform(0,np.pi*2)\n",
    "            rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],[np.sin(theta), np.cos(theta)]])\n",
    "            point_set[:,[0,2]] = point_set[:,[0,2]].dot(rotation_matrix) # random rotation\n",
    "            point_set += np.random.normal(0, 0.02, size=point_set.shape) # random jitter\n",
    "\n",
    "        seg = seg[choice]\n",
    "        point_set = torch.from_numpy(point_set)\n",
    "        seg = torch.from_numpy(seg)\n",
    "        cls = torch.from_numpy(np.array([cls]).astype(np.int64))\n",
    "\n",
    "        if self.classification:\n",
    "            return point_set, cls\n",
    "        else:\n",
    "            return point_set, seg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapath)\n",
    "\n",
    "class ModelNetDataset(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 npoints=2500,\n",
    "                 split='train',\n",
    "                 data_augmentation=True):\n",
    "        self.npoints = npoints\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.fns = []\n",
    "        with open(os.path.join(root, '{}.txt'.format(self.split)), 'r') as f:\n",
    "            for line in f:\n",
    "                self.fns.append(line.strip())\n",
    "\n",
    "        self.cat = {}\n",
    "        with open('./misc/modelnet_id.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                ls = line.strip().split()\n",
    "                self.cat[ls[0]] = int(ls[1])\n",
    "\n",
    "        print(self.cat)\n",
    "        self.classes = list(self.cat.keys())\n",
    "\n",
    "    def __getitem2__(self, index):\n",
    "      #impath, target = self.imlist[index]\n",
    "      myF = open(os.path.join(self.root, '{}.txt'.format(self.split)), 'r')\n",
    "      firstLine = myF.readline().strip()\n",
    "      if 'OFF' != firstLine[0:3]:\n",
    "          raise ValueError('Not a valid OFF header')\n",
    "      if len(firstLine) > 3:\n",
    "          n_verts, n_faces, n_edges = tuple([int(s) for s in firstLine[4:].split(' ')])\n",
    "      else:\n",
    "          n_verts, n_faces, n_edges = tuple([int(s) for s in myF.readline().strip().split(' ')])\n",
    "      verts = torch.tensor([[float(s) for s in myF.readline().strip().split(' ')] for i_vert in range(n_verts)])\n",
    "      #ipdb.set_trace(context=5)#\n",
    "      if verts.shape[0] > 0:\n",
    "        choice = np.random.choice(verts.shape[0], numpoints, replace=True)\n",
    "        point_set = verts[choice, :]\n",
    "      else:\n",
    "        point_set=verts \n",
    "      #faces = [[int(s) for s in myF.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n",
    "      return point_set, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fns)\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ###ipdb.set_trace(context=5)#\n",
    "        fn = self.fns[index]\n",
    "        cls = self.cat[fn.split('/')[0]]\n",
    "        #ipdb.set_trace(context=5)#\n",
    "\n",
    "        '''\n",
    "        with open(os.path.join(self.root, fn), 'rb') as f:\n",
    "            plydata = PlyData.read(f)\n",
    "        pts = np.vstack([plydata['vertex']['x'], plydata['vertex']['y'], plydata['vertex']['z']]).T\n",
    "        \n",
    "        '''\n",
    "        ipdb.set_trace(context=5)#\n",
    "        with open(os.path.join(self.root, fn), 'r') as myF:\n",
    "            firstLine = myF.readline().strip()\n",
    "            #if 'OFF' != firstLine[0:3]:\n",
    "            #    raise ValueError('Not a valid OFF header')\n",
    "            if len(firstLine) > 3:\n",
    "                n_verts, n_faces, n_edges = tuple([int(s) for s in firstLine[4:].split(' ')])\n",
    "            else:\n",
    "                n_verts, n_faces, n_edges = tuple([int(s) for s in myF.readline().strip().split(' ')])\n",
    "            pts = torch.tensor([[float(s) for s in myF.readline().strip().split(' ')] for i_vert in range(n_verts)])       \n",
    "\n",
    "        \n",
    "\n",
    "        choice = np.random.choice(len(pts), self.npoints, replace=True)\n",
    "        point_set = pts[choice, :]\n",
    "\n",
    "        point_set = point_set - np.expand_dims(np.mean(point_set, axis=0), 0)  # center\n",
    "        dist = np.max(np.sqrt(np.sum(point_set ** 2, axis=1)), 0)\n",
    "        point_set = point_set / dist  # scale\n",
    "\n",
    "        if self.data_augmentation:\n",
    "            theta = np.random.uniform(0, np.pi * 2)\n",
    "            rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "            point_set[:, [0, 2]] = point_set[:, [0, 2]].dot(rotation_matrix)  # random rotation\n",
    "            point_set += np.random.normal(0, 0.02, size=point_set.shape)  # random jitter\n",
    "\n",
    "        point_set = torch.from_numpy(point_set.astype(np.float32))\n",
    "        cls = torch.from_numpy(np.array([cls]).astype(np.int64))\n",
    "        return point_set, cls        \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = sys.argv[1]\n",
    "    datapath = sys.argv[2]\n",
    "\n",
    "    if dataset == 'shapenet':\n",
    "        d = ShapeNetDataset(root = datapath, class_choice = ['Chair'])\n",
    "        print(len(d))\n",
    "        ps, seg = d[0]\n",
    "        print(ps.size(), ps.type(), seg.size(),seg.type())\n",
    "\n",
    "        d = ShapeNetDataset(root = datapath, classification = True)\n",
    "        print(len(d))\n",
    "        ps, cls = d[0]\n",
    "        print(ps.size(), ps.type(), cls.size(),cls.type())\n",
    "        # get_segmentation_classes(datapath)\n",
    "\n",
    "    if dataset == 'modelnet':\n",
    "        gen_modelnet_id(datapath)\n",
    "        d = ModelNetDataset(root=datapath)\n",
    "        print(len(d))\n",
    "        print(d[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETPIH3i-uLHu",
    "outputId": "1fad5692-d7f9-470b-fe04-5e693cef5e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'bathtub': 1, 'bed': 2, 'bench': 3, 'bookshelf': 4, 'bottle': 5, 'bowl': 6, 'car': 7, 'chair': 8, 'cone': 9, 'cup': 10, 'curtain': 11, 'desk': 12, 'door': 13, 'dresser': 14, 'flower_pot': 15, 'glass_box': 16, 'guitar': 17, 'keyboard': 18, 'lamp': 19, 'laptop': 20, 'mantel': 21, 'monitor': 22, 'night_stand': 23, 'person': 24, 'piano': 25, 'plant': 26, 'radio': 27, 'range_hood': 28, 'sink': 29, 'sofa': 30, 'stairs': 31, 'stool': 32, 'table': 33, 'tent': 34, 'toilet': 35, 'tv_stand': 36, 'vase': 37, 'wardrobe': 38, 'xbox': 39}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class opt:\n",
    "  pass\n",
    "\n",
    "opt.batchSize=32\n",
    "opt.num_points=2500\n",
    "opt.workers=4\n",
    "opt.nepoch=10\n",
    "opt.outf='cls'\n",
    "opt.model=''\n",
    "opt.dataset='./ModelNet40'\n",
    "opt.dataset_type='modelnet40'\n",
    "'''\n",
    "opt.dataset='./shapenetcore_partanno_segmentation_benchmark_v0'\n",
    "opt.dataset_type='shapenet'\n",
    "'''\n",
    "dataset = ModelNetDataset(\n",
    "        root=opt.dataset,\n",
    "        npoints=opt.num_points)\n",
    "'''\n",
    "dataset = ShapeNetDataset(\n",
    "        root=opt.dataset,\n",
    "        classification=True,\n",
    "        npoints=opt.num_points)\n",
    "'''\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=opt.batchSize,\n",
    "    shuffle=True,\n",
    "    num_workers=int(opt.workers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVCRUnxeugnX"
   },
   "outputs": [],
   "source": [
    "dataloader.dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Me65ztfUn3Gk",
    "outputId": "004e665c-28b8-454a-c6e3-d265fbbf5fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.opt'>\n",
      "Random Seed:  9249\n",
      "{'airplane': 0, 'bathtub': 1, 'bed': 2, 'bench': 3, 'bookshelf': 4, 'bottle': 5, 'bowl': 6, 'car': 7, 'chair': 8, 'cone': 9, 'cup': 10, 'curtain': 11, 'desk': 12, 'door': 13, 'dresser': 14, 'flower_pot': 15, 'glass_box': 16, 'guitar': 17, 'keyboard': 18, 'lamp': 19, 'laptop': 20, 'mantel': 21, 'monitor': 22, 'night_stand': 23, 'person': 24, 'piano': 25, 'plant': 26, 'radio': 27, 'range_hood': 28, 'sink': 29, 'sofa': 30, 'stairs': 31, 'stool': 32, 'table': 33, 'tent': 34, 'toilet': 35, 'tv_stand': 36, 'vase': 37, 'wardrobe': 38, 'xbox': 39}\n",
      "{'airplane': 0, 'bathtub': 1, 'bed': 2, 'bench': 3, 'bookshelf': 4, 'bottle': 5, 'bowl': 6, 'car': 7, 'chair': 8, 'cone': 9, 'cup': 10, 'curtain': 11, 'desk': 12, 'door': 13, 'dresser': 14, 'flower_pot': 15, 'glass_box': 16, 'guitar': 17, 'keyboard': 18, 'lamp': 19, 'laptop': 20, 'mantel': 21, 'monitor': 22, 'night_stand': 23, 'person': 24, 'piano': 25, 'plant': 26, 'radio': 27, 'range_hood': 28, 'sink': 29, 'sofa': 30, 'stairs': 31, 'stool': 32, 'table': 33, 'tent': 34, 'toilet': 35, 'tv_stand': 36, 'vase': 37, 'wardrobe': 38, 'xbox': 39}\n",
      "7863 2468\n",
      "classes 40\n",
      "debug1\n",
      "debug2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/lib/python3.8/bdb.py\", line 334, in set_trace\n",
      "    sys.settrace(self.trace_dispatch)\n",
      "\n",
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/lib/python3.8/bdb.py\", line 334, in set_trace\n",
      "    sys.settrace(self.trace_dispatch)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-2-27efe78c0493>\u001b[0m(205)\u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    203 \u001b[0;31m        '''\n",
      "\u001b[0m\u001b[0;32m    204 \u001b[0;31m        \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 205 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    206 \u001b[0;31m            \u001b[0mfirstLine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    207 \u001b[0;31m            \u001b[0;31m#if 'OFF' != firstLine[0:3]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m<ipython-input-2-27efe78c0493>\u001b[0m(205)\u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    203 \u001b[0;31m        '''\n",
      "\u001b[0m\u001b[0;32m    204 \u001b[0;31m        \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 205 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    206 \u001b[0;31m            \u001b[0mfirstLine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    207 \u001b[0;31m            \u001b[0;31m#if 'OFF' != firstLine[0:3]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/lib/python3.8/bdb.py\", line 334, in set_trace\n",
      "    sys.settrace(self.trace_dispatch)\n",
      "\n",
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/lib/python3.8/bdb.py\", line 334, in set_trace\n",
      "    sys.settrace(self.trace_dispatch)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-2-27efe78c0493>\u001b[0m(205)\u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    203 \u001b[0;31m        '''\n",
      "\u001b[0m\u001b[0;32m    204 \u001b[0;31m        \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 205 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    206 \u001b[0;31m            \u001b[0mfirstLine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    207 \u001b[0;31m            \u001b[0;31m#if 'OFF' != firstLine[0:3]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m> \u001b[0;32m<ipython-input-2-27efe78c0493>\u001b[0m(205)\u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    203 \u001b[0;31m        '''\n",
      "\u001b[0m\u001b[0;32m    204 \u001b[0;31m        \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 205 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    206 \u001b[0;31m            \u001b[0mfirstLine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    207 \u001b[0;31m            \u001b[0;31m#if 'OFF' != firstLine[0:3]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/lib/python3.8/bdb.py\", line 359, in set_quit\n",
      "    sys.settrace(None)\n",
      "\n",
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/debugger.py\", line 974, in cmdloop\n",
      "    sys.settrace(None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--KeyboardInterrupt--"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/lib/python3.8/bdb.py\", line 359, in set_quit\n",
      "    sys.settrace(None)\n",
      "\n",
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/lib/python3.8/bdb.py\", line 359, in set_quit\n",
      "    sys.settrace(None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/debugger.py\", line 974, in cmdloop\n",
      "    sys.settrace(None)\n",
      "\n",
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/lib/python3.8/bdb.py\", line 359, in set_quit\n",
      "    sys.settrace(None)\n",
      "\n",
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/debugger.py\", line 974, in cmdloop\n",
      "    sys.settrace(None)\n",
      "\n",
      "\n",
      "PYDEV DEBUGGER WARNING:\n",
      "sys.settrace() should not be used when the debugger is being used.\n",
      "This may cause the debugger to stop working correctly.\n",
      "If this is needed, please check: \n",
      "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
      "to see how to restore the debug tracing back correctly.\n",
      "Call Location:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/debugger.py\", line 974, in cmdloop\n",
      "    sys.settrace(None)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "> \u001b[0;32m<ipython-input-2-27efe78c0493>\u001b[0m(205)\u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    203 \u001b[0;31m        '''\n",
      "\u001b[0m\u001b[0;32m    204 \u001b[0;31m        \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 205 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    206 \u001b[0;31m            \u001b[0mfirstLine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    207 \u001b[0;31m            \u001b[0;31m#if 'OFF' != firstLine[0:3]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m<ipython-input-2-27efe78c0493>\u001b[0m(205)\u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    203 \u001b[0;31m        '''\n",
      "\u001b[0m\u001b[0;32m    204 \u001b[0;31m        \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 205 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    206 \u001b[0;31m            \u001b[0mfirstLine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    207 \u001b[0;31m            \u001b[0;31m#if 'OFF' != firstLine[0:3]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m> \u001b[0;32m<ipython-input-2-27efe78c0493>\u001b[0m(205)\u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    203 \u001b[0;31m        '''\n",
      "\u001b[0m\u001b[0;32m    204 \u001b[0;31m        \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 205 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    206 \u001b[0;31m            \u001b[0mfirstLine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    207 \u001b[0;31m            \u001b[0;31m#if 'OFF' != firstLine[0:3]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\n",
      "> \u001b[0;32m<ipython-input-2-27efe78c0493>\u001b[0m(205)\u001b[0;36m__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    203 \u001b[0;31m        '''\n",
      "\u001b[0m\u001b[0;32m    204 \u001b[0;31m        \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 205 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    206 \u001b[0;31m            \u001b[0mfirstLine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    207 \u001b[0;31m            \u001b[0;31m#if 'OFF' != firstLine[0:3]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 131\u001b[0m\n\u001b[1;32m    129\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#ipdb.set_trace(context=5)#\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m#ipdb.set_trace(context=5)#\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     points, target \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    134\u001b[0m     target \u001b[38;5;241m=\u001b[39m target[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1284\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "#from pointnet.dataset import ShapeNetDataset, ModelNetDataset\n",
    "#from pointnet.model import PointNetCls, feature_transform_regularizer\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "#ipdb.set_trace(context=5)\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\n",
    "#    '--batchSize', type=int, default=32, help='input batch size')\n",
    "#parser.add_argument(\n",
    "#    '--num_points', type=int, default=2500, help='input batch size')\n",
    "#parser.add_argument(\n",
    "#    '--workers', type=int, help='number of data loading workers', default=4)\n",
    "#parser.add_argument(\n",
    "#    '--nepoch', type=int, default=250, help='number of epochs to train for')\n",
    "#parser.add_argument('--outf', type=str, default='cls', help='output folder')\n",
    "#parser.add_argument('--model', type=str, default='', help='model path')\n",
    "#parser.add_argument('--dataset', type=str, required=True, help=\"dataset path\")\n",
    "#parser.add_argument('--dataset_type', type=str, default='shapenet', help=\"dataset type shapenet|modelnet40\")\n",
    "#parser.add_argument('--feature_transform', action='store_true', help=\"use feature transform\")\n",
    "\n",
    "#ipdb.set_trace(context=5)\n",
    "#parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "#opt = parser.parse_args()\n",
    "\n",
    "class opt:\n",
    "  pass\n",
    "\n",
    "opt.batchSize=32\n",
    "opt.num_points=2500\n",
    "opt.workers=4\n",
    "opt.nepoch=10\n",
    "opt.outf='cls'\n",
    "opt.model=''\n",
    "\n",
    "'''\n",
    "opt.dataset='./shapenetcore_partanno_segmentation_benchmark_v0'\n",
    "opt.dataset_type='shapenet'\n",
    "'''\n",
    "opt.dataset='./ModelNet40'\n",
    "opt.dataset_type='modelnet40'\n",
    "\n",
    "\n",
    "opt.feature_transform='store_true'\n",
    "\n",
    "\n",
    "print(opt)\n",
    "\n",
    "blue = lambda x: '\\033[94m' + x + '\\033[0m'\n",
    "\n",
    "opt.manualSeed = random.randint(1, 10000)  # fix seed\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "if opt.dataset_type == 'shapenet':\n",
    "    dataset = ShapeNetDataset(\n",
    "        root=opt.dataset,\n",
    "        classification=True,\n",
    "        npoints=opt.num_points)\n",
    "\n",
    "    test_dataset = ShapeNetDataset(\n",
    "        root=opt.dataset,\n",
    "        classification=True,\n",
    "        split='test',\n",
    "        npoints=opt.num_points,\n",
    "        data_augmentation=False)\n",
    "elif opt.dataset_type == 'modelnet40':\n",
    "    dataset = ModelNetDataset(\n",
    "        root=opt.dataset,\n",
    "        npoints=opt.num_points,\n",
    "        split='train')\n",
    "\n",
    "    test_dataset = ModelNetDataset(\n",
    "        root=opt.dataset,\n",
    "        split='test',\n",
    "        npoints=opt.num_points,\n",
    "        data_augmentation=False)\n",
    "else:\n",
    "    exit('wrong dataset type')\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=opt.batchSize,\n",
    "    shuffle=True,\n",
    "    num_workers=int(opt.workers))\n",
    "\n",
    "testdataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=opt.batchSize,\n",
    "        shuffle=True,\n",
    "        num_workers=int(opt.workers))\n",
    "\n",
    "print(len(dataset), len(test_dataset))\n",
    "num_classes = len(dataset.classes)\n",
    "print('classes', num_classes)\n",
    "\n",
    "try:\n",
    "    os.makedirs(opt.outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "classifier = PointNetCls(k=num_classes, feature_transform=opt.feature_transform)\n",
    "\n",
    "if opt.model != '':\n",
    "    classifier.load_state_dict(torch.load(opt.model))\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "classifier.cuda()\n",
    "\n",
    "num_batch = len(dataset) / opt.batchSize\n",
    "\n",
    "\n",
    "#ipdb.set_trace(context=5)#\n",
    "for epoch in range(opt.nepoch):\n",
    "    scheduler.step()\n",
    "    #ipdb.set_trace(context=5)#\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        #ipdb.set_trace(context=5)#\n",
    "        points, target = data\n",
    "        target = target[:, 0]\n",
    "        points = points.transpose(2, 1)\n",
    "        points, target = points.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        classifier = classifier.train()\n",
    "        pred, trans, trans_feat = classifier(points)\n",
    "        #ipdb.set_trace(context=5)#\n",
    "        loss = F.nll_loss(pred, target)\n",
    "        if opt.feature_transform:\n",
    "            loss += feature_transform_regularizer(trans_feat) * 0.001\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "        correct = pred_choice.eq(target.data).cpu().sum()\n",
    "        print('[%d: %d/%d] train loss: %f accuracy: %f' % (epoch, i, num_batch, loss.item(), correct.item() / float(opt.batchSize)))\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            j, data = next(enumerate(testdataloader, 0))\n",
    "            points, target = data\n",
    "            target = target[:, 0]\n",
    "            points = points.transpose(2, 1)\n",
    "            points, target = points.cuda(), target.cuda()\n",
    "            classifier = classifier.eval()\n",
    "            pred, _, _ = classifier(points)\n",
    "            loss = F.nll_loss(pred, target)\n",
    "            pred_choice = pred.data.max(1)[1]\n",
    "            correct = pred_choice.eq(target.data).cpu().sum()\n",
    "            print('[%d: %d/%d] %s loss: %f accuracy: %f' % (epoch, i, num_batch, blue('test'), loss.item(), correct.item()/float(opt.batchSize)))\n",
    "\n",
    "    torch.save(classifier.state_dict(), '%s/cls_model_%d.pth' % (opt.outf, epoch))\n",
    "\n",
    "total_correct = 0\n",
    "total_testset = 0\n",
    "for i,data in tqdm(enumerate(testdataloader, 0)):\n",
    "    points, target = data\n",
    "    target = target[:, 0]\n",
    "    points = points.transpose(2, 1)\n",
    "    points, target = points.cuda(), target.cuda()\n",
    "    classifier = classifier.eval()\n",
    "    pred, _, _ = classifier(points)\n",
    "    pred_choice = pred.data.max(1)[1]\n",
    "    correct = pred_choice.eq(target.data).cpu().sum()\n",
    "    total_correct += correct.item()\n",
    "    total_testset += points.size()[0]\n",
    "\n",
    "print(\"final accuracy {}\".format(total_correct / float(total_testset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGtjjY5xNV1F",
    "outputId": "fea56c31-2873-40ee-853b-7c3f6aabd112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 13 14:31:39 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   65C    P0    30W /  70W |   7650MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AyT2T2o8st4F",
    "outputId": "d67cb8fb-ca15-4d0a-c5d7-e0d061ac4b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sofa_0646.off\n"
     ]
    }
   ],
   "source": [
    "!ls ./ModelNet40/sofa/train | grep sofa_0646.off"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
